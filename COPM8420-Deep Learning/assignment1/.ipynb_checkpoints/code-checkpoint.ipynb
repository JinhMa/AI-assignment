{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and Parameters\n",
    "\n",
    "n_features = 64\n",
    "num_classes = 10\n",
    "\n",
    "data = pd.read_csv('dataset/optdigits.tra.csv',  header=None)\n",
    "train_input = data.iloc[:, :n_features].as_matrix()\n",
    "train_target = data.iloc[:, n_features].as_matrix()\n",
    "num_train = train_input.shape[0]\n",
    "X = Variable(torch.Tensor(train_input).float())\n",
    "Y = Variable(torch.Tensor(train_target).long())\n",
    "\n",
    "data_test = pd.read_csv('dataset/optdigits.tes.csv',  header=None)\n",
    "train_input_test = data_test.iloc[:, :n_features].as_matrix()\n",
    "train_target_test = data_test.iloc[:, n_features].as_matrix()\n",
    "X_test = Variable(torch.Tensor(train_input_test).float())\n",
    "Y_test = Variable(torch.Tensor(train_target_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a two layer neural network\n",
    "# Function hidden_layer help to get hidden layer result\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        hidden_unit = out\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def hidden_layer(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function: notice that for each epoch, I will use the current \n",
    "# neural network to get losses and accuracy for test set. I will\n",
    "# keep all result in some lists. Therefore, I can easily plot all \n",
    "# losses and accuracy for train set and test set, which will help us\n",
    "# to see trend of them.\n",
    "\n",
    "def train(net, num_epochs, X, Y, if_plot=True):\n",
    "    print('Starting training...')\n",
    "    print('Hidden units:', hidden_size)\n",
    "    print('Number of epochs:', num_epochs)\n",
    "    print('==============================')\n",
    "    print('Progress: ', end='')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % (num_epochs / 20) == 0:\n",
    "            print('#', end='')\n",
    "        Y_pred = net(X)\n",
    "        Y_pred_test = net(X_test)\n",
    "\n",
    "        loss = criterion(Y_pred, Y)\n",
    "        loss_test = criterion(Y_pred_test, Y_test)\n",
    "\n",
    "        train_losses.append(loss.data[0])\n",
    "        test_losses.append(loss_test.data[0])\n",
    "\n",
    "        _, predicted = torch.max(Y_pred, 1)\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.data.numpy() == Y.data.numpy()\n",
    "        train_accuracy.append(100 * sum(correct)/total)\n",
    "\n",
    "        _, predicted_test = torch.max(Y_pred_test, 1)\n",
    "        total_test = predicted_test.size(0)\n",
    "        correct_test = predicted_test.data.numpy() == Y_test.data.numpy()\n",
    "        test_accuracy.append(100 * sum(correct_test)/total_test)\n",
    "\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # plot the losses and acuracy of train set and test set from each epoch.\n",
    "    if if_plot:\n",
    "        plt_result(train_losses, test_losses, train_accuracy, test_accuracy)\n",
    "    # print final accuracy for train set and test set\n",
    "    print()\n",
    "    print('Final train set accuracy: {0:.4f}%'.format(train_accuracy[-1]))\n",
    "    print('Final test set accuracy: {0:.4f}%'.format(test_accuracy[-1]))\n",
    "\n",
    "def test(net, X_test, Y_test):\n",
    "    Y_pred_test = net(X_test)\n",
    "    _, predicted_test = torch.max(Y_pred_test, 1)\n",
    "    total_test = predicted_test.size(0)\n",
    "    correct_test = sum(predicted_test.data.numpy() == Y_test.data.numpy())\n",
    "    return 100 * correct_test / total_test\n",
    "    \n",
    "def plt_result(train_losses, test_losses, train_accuracy, test_accuracy):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('cost')\n",
    "    plt.plot(train_losses, label='train set')\n",
    "    plt.plot(test_losses, label='test set')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(train_accuracy, label='train set')\n",
    "    plt.plot(test_accuracy, label='test set')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmS6n4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115a1eb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.3829%\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "\n",
    "hidden_size = 100\n",
    "num_epochs = 1000\n",
    "net = Net(n_features, hidden_size, num_classes)\n",
    "train(net, num_epochs, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove redundant units. Notice that threshold \n",
    "# and hidden_size are parameters\n",
    "\n",
    "def remove_redundant_units(net, hidden_size, X, threshold=15):\n",
    "    hidden_units = net.hidden_layer(X).data.numpy()\n",
    "    weight2 = net.fc2.weight.data.numpy()\n",
    "    removed = set()\n",
    "    num_removed = 0\n",
    "    \n",
    "    for i in range(hidden_size):\n",
    "        a = hidden_units[:, i].reshape(-1, 1)\n",
    "        if np.linalg.norm(a) < num_train * 0.01:\n",
    "            print('hidden unit: {0:2d}   norm: {1:.2f}'.format(i, np.linalg.norm(a)))\n",
    "            removed.add(i)\n",
    "            weight2[:, i].fill(0)\n",
    "            num_removed += 1\n",
    "    else:\n",
    "        print('There is no unit close to 0')\n",
    "\n",
    "    print('   units      angle')\n",
    "    for i in range(hidden_size):\n",
    "        if i in removed:\n",
    "            continue\n",
    "        a = hidden_units[:, i].reshape(-1, 1)\n",
    "        for j in range(i+1, hidden_size):\n",
    "            if j in removed:\n",
    "                continue\n",
    "            b = hidden_units[:, j].reshape(-1, 1)\n",
    "            cos = np.asscalar(a.T @ b / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "            cos = min(cos, 1)\n",
    "            cos = max(cos, -1)\n",
    "            angle = np.arccos(cos) * 180 / np.pi\n",
    "            if angle < threshold:\n",
    "                print('{0:4d} {1:4d}    {2:.4f}'.format(i, j, angle))\n",
    "                removed.add(j)\n",
    "                weight2[:, i] += weight2[:, j]\n",
    "                weight2[:, j].fill(0)\n",
    "                num_removed += 1\n",
    "            if angle > 180 - threshold:\n",
    "                print('{0:4d} {1:4d}    {2:.4f}'.format(i, j, angle))\n",
    "                removed.add(j)\n",
    "                weight2[:, i] -= weight2[:, j]\n",
    "                weight2[:, j].fill(0)\n",
    "                num_removed += 1\n",
    "                break\n",
    "\n",
    "    return weight2, num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.9738%\n",
      "Final test set accuracy: 95.8820%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  18   33    179.8629\n",
      "Number of removed units: 1\n",
      "After remove units, test set ccuracy: 95.8820%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.8954%\n",
      "Final test set accuracy: 95.8820%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   23    29.3004\n",
      "   5   28    153.0389\n",
      "  15   35    0.3220\n",
      "  15   39    0.0766\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.0490%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.9215%\n",
      "Final test set accuracy: 96.4942%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    4    138.0682\n",
      "   1   13    38.3025\n",
      "   1   32    38.3019\n",
      "   1   36    140.3568\n",
      "   3   11    140.4073\n",
      "   6   21    40.1346\n",
      "  12   14    136.1380\n",
      "  16   23    138.1565\n",
      "  31   34    144.7804\n",
      "  39   42    26.9032\n",
      "  43   44    143.7082\n",
      "Number of removed units: 11\n",
      "After remove units, test set ccuracy: 95.7151%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6055%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  28   34    175.8308\n",
      "  63   70    0.7412\n",
      "  63   75    1.3749\n",
      "  63   77    0.5908\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.6055%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  10   64    160.9522\n",
      "  24   39    157.5220\n",
      "  40   41    0.3000\n",
      "  40   63    179.9604\n",
      "  65   66    23.4861\n",
      "  67   74    0.5238\n",
      "  67   92    0.1454\n",
      "  67   98    180.0000\n",
      "Number of removed units: 8\n",
      "After remove units, test set ccuracy: 96.6055%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.3272%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   15    39.0029\n",
      "   2   20    162.9961\n",
      "   3   30    42.0288\n",
      "   4   24    42.2545\n",
      "   7   44    38.9579\n",
      "  11   23    44.7072\n",
      "  13   55    34.6338\n",
      "  14   43    36.0381\n",
      "  14   54    36.8684\n",
      "  14   74    146.3981\n",
      "  16   93    143.3076\n",
      "  18   19    148.1921\n",
      "  28   60    36.0127\n",
      "  32   33    135.9783\n",
      "  34   62    34.7971\n",
      "  36   38    0.0000\n",
      "  36   41    179.1377\n",
      "  47   58    0.3232\n",
      "  47   59    0.5585\n",
      "  47   71    179.6478\n",
      "  73   77    179.8813\n",
      "  82   96    158.7057\n",
      "  83   99    139.1754\n",
      "Number of removed units: 23\n",
      "After remove units, test set ccuracy: 96.2159%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 97.0506%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2    8    179.8493\n",
      "  20   34    166.2552\n",
      "  37   59    0.1998\n",
      "  37   71    174.9580\n",
      "  95  108    177.9988\n",
      "Number of removed units: 5\n",
      "After remove units, test set ccuracy: 97.0506%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.3829%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   3    6    180.0000\n",
      "  13   30    2.7611\n",
      "  13   31    172.1581\n",
      "  38   41    151.9781\n",
      "  39  125    28.0439\n",
      "  46   47    1.1975\n",
      "  46   49    1.9342\n",
      "  46   51    0.8891\n",
      "  46   77    179.4436\n",
      "  64  142    153.0453\n",
      "  80   99    2.2214\n",
      "  80  107    0.2974\n",
      "  80  111    0.4083\n",
      "  80  147    167.2133\n",
      "Number of removed units: 14\n",
      "After remove units, test set ccuracy: 96.4942%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.5498%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   1   24    44.1291\n",
      "   3   12    135.6986\n",
      "   5  137    44.4968\n",
      "   6  101    39.1243\n",
      "   7   47    41.9384\n",
      "   7   50    40.0161\n",
      "   7  107    32.9487\n",
      "   7  111    44.4459\n",
      "   7  132    138.5629\n",
      "  14   56    144.6747\n",
      "  15   98    42.5669\n",
      "  15  122    44.1028\n",
      "  15  145    136.3354\n",
      "  16   83    135.5933\n",
      "  17  102    40.9480\n",
      "  19  121    42.4240\n",
      "  20  147    146.4527\n",
      "  21   23    179.8533\n",
      "  22   84    144.9764\n",
      "  25   33    36.7581\n",
      "  25   38    138.0876\n",
      "  26  134    138.1670\n",
      "  27  106    135.7373\n",
      "  34   45    38.3165\n",
      "  35   97    136.3878\n",
      "  40   71    44.0639\n",
      "  41  120    44.2225\n",
      "  41  136    33.2484\n",
      "  42   44    179.8155\n",
      "  46  149    138.2966\n",
      "  48  119    39.1084\n",
      "  53   63    136.9385\n",
      "  55   66    40.4046\n",
      "  55   67    0.1009\n",
      "  55   86    0.3369\n",
      "  55   90    0.0000\n",
      "  55   91    2.4965\n",
      "  55   92    0.0000\n",
      "  55   95    167.8419\n",
      "  58  124    40.0734\n",
      "  59   64    144.0301\n",
      "  62   85    135.3166\n",
      "  68  142    43.9771\n",
      "  74   88    39.7895\n",
      "  74   99    33.7459\n",
      "  74  108    42.0321\n",
      "  76  140    40.5865\n",
      "  94  143    136.6823\n",
      "  96  100    43.9702\n",
      "  96  117    39.4386\n",
      "  96  131    39.0260\n",
      "  96  138    139.1518\n",
      " 104  114    35.7736\n",
      " 127  133    31.7429\n",
      "Number of removed units: 54\n",
      "After remove units, test set ccuracy: 96.0490%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.4942%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  14   15    178.0142\n",
      "  89  108    176.8909\n",
      " 117  148    171.8921\n",
      " 165  176    179.5652\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.4942%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 97.1063%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   91    25.3780\n",
      "   4    6    0.8904\n",
      "   4   11    166.2393\n",
      "  16   26    1.0706\n",
      "  16   27    2.2084\n",
      "  16   39    179.9138\n",
      "  45   63    4.2181\n",
      "  45   78    160.3636\n",
      "  54   79    18.9745\n",
      "  74  125    26.3672\n",
      "  81   84    20.8893\n",
      "  81   88    0.2639\n",
      "  81   99    0.4681\n",
      "  81  128    0.4731\n",
      "  81  135    10.7237\n",
      "  81  144    153.1367\n",
      " 111  112    25.4200\n",
      " 111  160    22.9645\n",
      " 151  153    169.0926\n",
      " 171  172    179.6816\n",
      " 178  179    0.0198\n",
      " 178  185    179.7602\n",
      "Number of removed units: 22\n",
      "After remove units, test set ccuracy: 96.9950%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.5498%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0   89    43.8692\n",
      "   2   11    18.1645\n",
      "   2   12    157.4550\n",
      "   3  101    138.6187\n",
      "   4   28    41.9417\n",
      "   4   40    37.1756\n",
      "   4   72    137.0553\n",
      "   9   80    41.5541\n",
      "   9  172    39.1487\n",
      "   9  174    136.4114\n",
      "  15  151    41.9287\n",
      "  16  122    143.4428\n",
      "  18   34    137.6992\n",
      "  19  164    43.2978\n",
      "  20   38    136.7333\n",
      "  22  125    43.0155\n",
      "  22  131    135.0877\n",
      "  23   27    139.7544\n",
      "  30   52    6.2958\n",
      "  30   53    173.7058\n",
      "  33   43    39.0860\n",
      "  35  171    37.4313\n",
      "  36   39    43.8728\n",
      "  36  128    34.4097\n",
      "  36  157    135.0825\n",
      "  41   64    44.3683\n",
      "  41  108    153.0918\n",
      "  48  179    41.4409\n",
      "  49  156    135.3241\n",
      "  57   73    41.7315\n",
      "  58   61    168.9644\n",
      "  59  186    137.1747\n",
      "  66  147    43.5007\n",
      "  66  152    42.6015\n",
      "  66  176    138.1709\n",
      "  69   86    142.9045\n",
      "  75   78    13.3335\n",
      "  75   82    139.7450\n",
      "  83  110    41.9625\n",
      "  83  193    144.1583\n",
      "  84   90    138.4462\n",
      "  85   87    43.3945\n",
      "  92  135    41.0541\n",
      "  94  139    139.6749\n",
      "  98  149    138.1133\n",
      " 104  114    141.9572\n",
      " 111  191    40.9052\n",
      " 116  119    174.1863\n",
      " 117  120    156.0817\n",
      " 124  129    1.3763\n",
      " 124  132    175.6351\n",
      " 127  150    34.0595\n",
      " 127  188    44.9626\n",
      " 138  153    43.9941\n",
      " 138  170    136.0051\n",
      " 142  180    145.9209\n",
      " 145  168    42.0344\n",
      " 154  190    44.2295\n",
      " 163  198    142.8074\n",
      "Number of removed units: 59\n",
      "After remove units, test set ccuracy: 96.3272%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do some experiment\n",
    "\n",
    "num_epochs = 1000\n",
    "for hidden_size in [50, 100, 150, 200]:\n",
    "    for threshold in [15, 30, 45]:\n",
    "        print('Threshold:', threshold)\n",
    "        net = Net(n_features, hidden_size, num_classes)\n",
    "        train(net, num_epochs, X, Y, if_plot=False)\n",
    "        original_weight2 = net.fc2.weight.data.numpy()\n",
    "        weight2, num_removed = remove_redundant_units(net, hidden_size, X, threshold=threshold)\n",
    "        print('Number of removed units:', num_removed)\n",
    "        net.fc2.weight.data = torch.from_numpy(weight2)\n",
    "        print('After remove units, test set ccuracy: {0:.4f}%'.format(test(net, X_test, Y_test)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 60\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.9393%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    3    133.4056\n",
      "   1    5    134.0296\n",
      "   2   37    121.5175\n",
      "   4   16    127.4017\n",
      "   6   19    128.5839\n",
      "   7   13    53.4266\n",
      "   7   15    126.5679\n",
      "   8   34    48.3320\n",
      "   8   42    51.6732\n",
      "   8   48    126.1983\n",
      "   9   31    55.8454\n",
      "   9   33    126.0322\n",
      "  10   21    120.1338\n",
      "  11   75    50.9888\n",
      "  11   93    124.6787\n",
      "  12   28    121.5131\n",
      "  14   22    130.8649\n",
      "  17   18    135.1267\n",
      "  20   26    179.7113\n",
      "  23   25    124.0251\n",
      "  24   82    126.8240\n",
      "  27   58    127.3123\n",
      "  30   55    122.8562\n",
      "  32   35    1.2529\n",
      "  32   43    34.3910\n",
      "  32   44    1.2532\n",
      "  32   45    1.2523\n",
      "  32   47    55.3486\n",
      "  32   50    36.8811\n",
      "  32   51    1.2686\n",
      "  32   52    178.7465\n",
      "  36   64    54.2343\n",
      "  36   68    52.7664\n",
      "  36   90    57.8454\n",
      "  38   49    48.9417\n",
      "  38   66    125.9545\n",
      "  39   54    55.1479\n",
      "  39   65    49.9118\n",
      "  39   74    121.2444\n",
      "  40   46    121.6037\n",
      "  41   77    124.2384\n",
      "  53   60    125.6078\n",
      "  56   61    45.7969\n",
      "  56   72    130.2282\n",
      "  57   71    130.9512\n",
      "  59   63    57.7507\n",
      "  59   83    58.3850\n",
      "  59   86    53.2555\n",
      "  59   94    54.3310\n",
      "  59   99    48.1788\n",
      "  62   73    123.8689\n",
      "  67   92    41.3025\n",
      "  69   78    120.0203\n",
      "  76   80    179.4822\n",
      "  79   98    125.1238\n",
      "  84   85    136.3374\n",
      "  87   89    49.1592\n",
      "  88   95    121.8789\n",
      "  91   96    134.4193\n",
      "Number of removed units: 59\n",
      "After remove units, test set ccuracy: 94.4352%\n",
      "\n",
      "Threshold: 80\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    78.2077\n",
      "   0    2    69.7808\n",
      "   0    5    76.8759\n",
      "   0    6    76.4085\n",
      "   0    7    67.9247\n",
      "   0    8    70.1740\n",
      "   0    9    100.7167\n",
      "   3    4    101.5396\n",
      "  10   12    14.5673\n",
      "  10   16    76.1444\n",
      "  10   17    105.0092\n",
      "  11   14    111.7055\n",
      "  13   15    110.8620\n",
      "  18   22    112.3115\n",
      "  19   20    117.8926\n",
      "  21   23    112.3977\n",
      "  24   25    104.6142\n",
      "  26   30    108.0110\n",
      "  27   28    177.6554\n",
      "  29   31    117.0227\n",
      "  32   35    108.1169\n",
      "  33   34    110.9107\n",
      "  36   38    78.1554\n",
      "  36   39    112.4371\n",
      "  37   40    75.0375\n",
      "  37   41    114.8911\n",
      "  42   43    104.7660\n",
      "  44   45    63.7867\n",
      "  44   49    110.9521\n",
      "  46   47    77.0492\n",
      "  46   48    127.5370\n",
      "  50   51    111.6519\n",
      "  52   55    66.7132\n",
      "  52   59    103.7094\n",
      "  53   54    176.3569\n",
      "  56   62    102.6741\n",
      "  57   60    134.4297\n",
      "  58   65    101.0284\n",
      "  61   64    103.1845\n",
      "  63   67    76.5771\n",
      "  63   68    100.0026\n",
      "  66   69    128.8450\n",
      "  70   71    108.5168\n",
      "  72   73    69.7555\n",
      "  72   76    57.4129\n",
      "  72   77    75.5418\n",
      "  72   78    100.6834\n",
      "  74   75    58.8706\n",
      "  74   79    58.5408\n",
      "  74   80    76.8591\n",
      "  74   82    121.0118\n",
      "  81   83    104.1095\n",
      "  84   85    75.7031\n",
      "  84   87    120.8074\n",
      "  86   88    122.8057\n",
      "  89   90    75.3569\n",
      "  89   91    59.4356\n",
      "  89   92    78.7778\n",
      "  89   94    107.6431\n",
      "  93   97    122.8893\n",
      "  95   96    74.9716\n",
      "Number of removed units: 61\n",
      "After remove units, test set ccuracy: 89.2599%\n",
      "\n",
      "Threshold: 60\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    138.1146\n",
      "   2   22    57.5867\n",
      "   2   77    125.1831\n",
      "   3   39    120.3358\n",
      "   4   17    121.9770\n",
      "   5   42    121.4830\n",
      "   6   31    120.6770\n",
      "   7   51    58.5487\n",
      "   7   61    130.7127\n",
      "   8   10    129.1380\n",
      "   9   47    130.4159\n",
      "  11   14    125.8992\n",
      "  12   34    58.4027\n",
      "  12   92    134.8660\n",
      "  13   15    142.9059\n",
      "  16   19    125.1830\n",
      "  18   68    52.5150\n",
      "  18   71    51.3255\n",
      "  18  127    123.6532\n",
      "  20   28    122.7960\n",
      "  21   24    120.7606\n",
      "  23   56    58.7647\n",
      "  23   62    126.7026\n",
      "  25   27    126.6192\n",
      "  26   32    175.5658\n",
      "  29   82    58.8349\n",
      "  29   94    54.9490\n",
      "  29  147    45.8997\n",
      "  29  170    123.2338\n",
      "  30   45    130.3668\n",
      "  33   53    128.3610\n",
      "  35   46    152.8349\n",
      "  36   44    59.0787\n",
      "  36   48    123.9912\n",
      "  37   38    59.6789\n",
      "  37   57    120.9943\n",
      "  40   79    41.4055\n",
      "  40  118    125.0921\n",
      "  41   52    126.5853\n",
      "  43   59    120.4032\n",
      "  49   67    128.3780\n",
      "  50   60    143.0016\n",
      "  54   87    123.1491\n",
      "  55   74    57.3650\n",
      "  55   78    52.4364\n",
      "  55   84    59.9306\n",
      "  55   86    124.4307\n",
      "  58   99    127.1632\n",
      "  63   65    59.7653\n",
      "  63   90    53.4924\n",
      "  63   95    46.3313\n",
      "  63  103    54.8734\n",
      "  63  111    55.6503\n",
      "  63  129    123.3693\n",
      "  64   96    53.9835\n",
      "  64  110    56.4225\n",
      "  64  169    125.1405\n",
      "  66   72    130.7186\n",
      "  69  114    57.6185\n",
      "  69  115    55.5776\n",
      "  69  122    137.4940\n",
      "  70  124    54.3167\n",
      "  70  139    56.9259\n",
      "  70  159    133.5383\n",
      "  73   75    125.5683\n",
      "  76   80    59.7157\n",
      "  76   88    126.1741\n",
      "  81   98    130.6975\n",
      "  83  158    53.1548\n",
      "  85   89    40.2189\n",
      "  85   97    46.0608\n",
      "  85  101    41.7576\n",
      "  85  104    130.5072\n",
      "  93  121    123.4199\n",
      " 100  128    121.0288\n",
      " 102  109    130.4829\n",
      " 105  108    59.0632\n",
      " 105  156    122.7617\n",
      " 106  142    54.7321\n",
      " 106  155    57.6890\n",
      " 107  112    127.3868\n",
      " 113  116    49.4673\n",
      " 113  126    129.6925\n",
      " 117  119    122.1431\n",
      " 120  138    126.6748\n",
      " 123  130    58.9774\n",
      " 123  182    57.6210\n",
      " 123  198    57.8832\n",
      " 125  134    179.5860\n",
      " 131  132    54.3088\n",
      " 131  136    124.9397\n",
      " 133  152    122.8483\n",
      " 135  146    170.9675\n",
      " 137  140    128.2238\n",
      " 145  151    131.3472\n",
      " 148  160    125.3412\n",
      " 149  167    52.4788\n",
      " 149  176    120.0477\n",
      " 150  157    121.1214\n",
      " 153  162    120.4912\n",
      " 154  171    41.1761\n",
      " 154  172    131.3916\n",
      " 163  166    120.1349\n",
      " 164  165    128.8948\n",
      " 173  179    123.9968\n",
      " 174  191    45.3487\n",
      " 175  177    47.1806\n",
      " 175  184    128.3062\n",
      " 178  183    121.8161\n",
      " 187  188    27.0521\n",
      " 187  190    123.0069\n",
      " 192  194    122.0766\n",
      " 193  196    122.1769\n",
      "Number of removed units: 113\n",
      "After remove units, test set ccuracy: 95.8264%\n",
      "\n",
      "Threshold: 80\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    104.7224\n",
      "   2    4    62.9582\n",
      "   2    6    112.9869\n",
      "   3    5    68.6474\n",
      "   3    7    102.7615\n",
      "   8   10    107.6774\n",
      "   9   11    69.4178\n",
      "   9   12    140.7066\n",
      "  13   14    135.1820\n",
      "  15   16    67.6254\n",
      "  15   18    73.9880\n",
      "  15   19    57.8047\n",
      "  15   20    76.8344\n",
      "  15   29    76.5586\n",
      "  15   30    102.4819\n",
      "  17   22    72.9239\n",
      "  17   23    58.0633\n",
      "  17   24    76.0168\n",
      "  17   25    70.9573\n",
      "  17   27    74.3784\n",
      "  17   33    120.4135\n",
      "  21   28    72.3168\n",
      "  21   35    76.8976\n",
      "  21   36    108.6133\n",
      "  26   31    78.8901\n",
      "  26   32    114.6994\n",
      "  34   38    74.1940\n",
      "  34   39    119.7559\n",
      "  37   40    101.9197\n",
      "  41   42    33.1530\n",
      "  41   44    109.4598\n",
      "  43   46    76.3043\n",
      "  43   47    104.4990\n",
      "  45   48    107.1389\n",
      "  49   50    74.4979\n",
      "  49   51    107.7807\n",
      "  52   53    107.8504\n",
      "  54   57    119.4889\n",
      "  55   56    75.5590\n",
      "  55   58    67.7626\n",
      "  55   59    73.1497\n",
      "  55   61    104.9490\n",
      "  60   63    47.9072\n",
      "  60   65    79.8199\n",
      "  60   66    112.4836\n",
      "  62   70    105.3123\n",
      "  64   68    63.0329\n",
      "  64   75    66.6435\n",
      "  64   79    79.8777\n",
      "  64   80    101.4112\n",
      "  67   69    65.7578\n",
      "  67   74    65.8998\n",
      "  67   77    78.3850\n",
      "  67   78    103.5854\n",
      "  71   76    37.4815\n",
      "  71   81    118.6966\n",
      "  72   73    120.2002\n",
      "  82   83    55.0238\n",
      "  82   85    102.0937\n",
      "  84   89    73.0983\n",
      "  84   95    109.5817\n",
      "  86   87    148.5716\n",
      "  88   90    103.2906\n",
      "  91   92    127.2069\n",
      "  93   94    73.3120\n",
      "  93   96    59.2210\n",
      "  93   97    79.7213\n",
      "  93  100    127.7782\n",
      "  98   99    111.3305\n",
      " 101  103    78.5325\n",
      " 101  104    79.4919\n",
      " 101  105    66.3201\n",
      " 101  107    104.8110\n",
      " 102  106    176.3400\n",
      " 108  110    105.6006\n",
      " 109  111    126.6232\n",
      " 112  114    110.9767\n",
      " 113  115    109.9322\n",
      " 116  119    107.0820\n",
      " 117  118    117.3960\n",
      " 120  123    64.6936\n",
      " 120  124    101.4916\n",
      " 121  126    78.7494\n",
      " 121  127    103.4730\n",
      " 122  125    102.8744\n",
      " 128  129    106.1636\n",
      " 130  133    104.7517\n",
      " 131  134    114.3407\n",
      " 132  135    63.2003\n",
      " 132  138    108.5463\n",
      " 136  140    107.7598\n",
      " 137  139    39.9699\n",
      " 137  143    102.4286\n",
      " 141  142    115.7755\n",
      " 144  148    103.0354\n",
      " 145  146    101.5185\n",
      " 147  149    67.1160\n",
      " 147  150    74.4683\n",
      " 147  151    75.1327\n",
      " 147  153    105.5293\n",
      " 152  157    103.0396\n",
      " 154  155    79.2521\n",
      " 154  158    77.3073\n",
      " 154  161    77.3425\n",
      " 154  162    109.7152\n",
      " 156  159    74.7900\n",
      " 156  160    59.7560\n",
      " 156  164    77.7373\n",
      " 156  165    79.9386\n",
      " 156  166    104.1027\n",
      " 163  168    78.8055\n",
      " 163  170    102.4073\n",
      " 167  171    109.9633\n",
      " 169  172    103.0527\n",
      " 173  174    101.6958\n",
      " 175  176    134.6502\n",
      " 177  178    100.5165\n",
      " 179  181    109.6372\n",
      " 180  182    70.7326\n",
      " 180  183    102.0263\n",
      " 184  185    106.0426\n",
      " 186  187    129.4376\n",
      " 188  189    61.9426\n",
      " 188  190    110.1568\n",
      " 191  192    103.4128\n",
      " 193  194    70.5793\n",
      " 193  195    101.6771\n",
      " 196  197    73.4081\n",
      " 196  199    100.5486\n",
      "Number of removed units: 129\n",
      "After remove units, test set ccuracy: 89.9833%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for hidden_size in [100,200]:\n",
    "    for threshold in [60, 80]:\n",
    "        print('Threshold:', threshold)\n",
    "        net = Net(n_features, hidden_size, num_classes)\n",
    "        train(net, num_epochs, X, Y, if_plot=False)\n",
    "        original_weight2 = net.fc2.weight.data.numpy()\n",
    "        weight2, num_removed = remove_redundant_units(net, hidden_size, X, threshold=threshold)\n",
    "        print('Number of removed units:', num_removed)\n",
    "        net.fc2.weight.data = torch.from_numpy(weight2)\n",
    "        print('After remove units, test set ccuracy: {0:.4f}%'.format(test(net, X_test, Y_test)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99 48 10 35 41 64 76 12 35 17 41 88 92 32 82 62 35 70 46 15 20 39 88 92 25\n",
      " 12 96 39 86 76 63 11 84 62  3 46 99 39 10 17 90 60 25 37  2 87 27 39 36 50]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 99 is out of bounds for axis 1 with size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d99482e0384b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moriginal_weight2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weight2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After remove units, test set ccuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 99 is out of bounds for axis 1 with size 50"
     ]
    }
   ],
   "source": [
    "# To compare this result with random remove units\n",
    "\n",
    "# hidden_size = 100\n",
    "# num_epochs = 1000\n",
    "# net = Net(n_features, hidden_size, num_classes)\n",
    "# train(net, num_epochs, X, Y, if_plot=False)\n",
    "# original_weight = net.fc2.weight.data.numpy()\n",
    "# random_num = np.random.randint(1, 100, 50)\n",
    "# print(random_num)\n",
    "# for i in random_num:\n",
    "#     original_weight2[:, i].fill(0)\n",
    "# net.fc2.weight.data = torch.from_numpy(original_weight2)\n",
    "# print('After remove units, test set ccuracy:', test(net, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
