{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and Parameters\n",
    "\n",
    "n_features = 64\n",
    "num_classes = 10\n",
    "\n",
    "data = pd.read_csv('dataset/optdigits.tra.csv',  header=None)\n",
    "train_input = data.iloc[:, :n_features].as_matrix()\n",
    "train_target = data.iloc[:, n_features].as_matrix()\n",
    "num_train = train_input.shape[0]\n",
    "X = Variable(torch.Tensor(train_input).float())\n",
    "Y = Variable(torch.Tensor(train_target).long())\n",
    "\n",
    "data_test = pd.read_csv('dataset/optdigits.tes.csv',  header=None)\n",
    "train_input_test = data_test.iloc[:, :n_features].as_matrix()\n",
    "train_target_test = data_test.iloc[:, n_features].as_matrix()\n",
    "X_test = Variable(torch.Tensor(train_input_test).float())\n",
    "Y_test = Variable(torch.Tensor(train_target_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a two layer neural network\n",
    "# Function hidden_layer help to get hidden layer result\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        hidden_unit = out\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def hidden_layer(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function: notice that for each epoch, I will use the current \n",
    "# neural network to get losses and accuracy for test set. I will\n",
    "# keep all result in some lists. Therefore, I can easily plot all \n",
    "# losses and accuracy for train set and test set, which will help us\n",
    "# to see trend of them.\n",
    "\n",
    "def train(net, num_epochs, X, Y, if_plot=True):\n",
    "    print('Starting training...')\n",
    "    print('Hidden units:', hidden_size)\n",
    "    print('Number of epochs:', num_epochs)\n",
    "    print('==============================')\n",
    "    print('Progress: ', end='')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % (num_epochs / 20) == 0:\n",
    "            print('#', end='')\n",
    "        Y_pred = net(X)\n",
    "        Y_pred_test = net(X_test)\n",
    "\n",
    "        loss = criterion(Y_pred, Y)\n",
    "        loss_test = criterion(Y_pred_test, Y_test)\n",
    "\n",
    "        train_losses.append(loss.data[0])\n",
    "        test_losses.append(loss_test.data[0])\n",
    "\n",
    "        _, predicted = torch.max(Y_pred, 1)\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.data.numpy() == Y.data.numpy()\n",
    "        train_accuracy.append(100 * sum(correct)/total)\n",
    "\n",
    "        _, predicted_test = torch.max(Y_pred_test, 1)\n",
    "        total_test = predicted_test.size(0)\n",
    "        correct_test = predicted_test.data.numpy() == Y_test.data.numpy()\n",
    "        test_accuracy.append(100 * sum(correct_test)/total_test)\n",
    "\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # plot the losses and acuracy of train set and test set from each epoch.\n",
    "    if if_plot:\n",
    "        plt_result(train_losses, test_losses, train_accuracy, test_accuracy)\n",
    "    # print final accuracy for train set and test set\n",
    "    print()\n",
    "    print('Final train set accuracy: {0:.4f}%'.format(train_accuracy[-1]))\n",
    "    print('Final test set accuracy: {0:.4f}%'.format(test_accuracy[-1]))\n",
    "\n",
    "def test(net, X_test, Y_test):\n",
    "    Y_pred_test = net(X_test)\n",
    "    _, predicted_test = torch.max(Y_pred_test, 1)\n",
    "    total_test = predicted_test.size(0)\n",
    "    correct_test = sum(predicted_test.data.numpy() == Y_test.data.numpy())\n",
    "    return 100 * correct_test / total_test\n",
    "    \n",
    "def plt_result(train_losses, test_losses, train_accuracy, test_accuracy):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('cost')\n",
    "    plt.plot(train_losses, label='train set')\n",
    "    plt.plot(test_losses, label='test set')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(train_accuracy, label='train set')\n",
    "    plt.plot(test_accuracy, label='test set')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 200\n",
      "==============================\n",
      "Progress: #"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaolongfei/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/zhaolongfei/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvckAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1138046d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final train set accuracy: 98.9014%\n",
      "Final test set accuracy: 95.3255%\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "\n",
    "hidden_size = 100\n",
    "num_epochs = 200\n",
    "net = Net(n_features, hidden_size, num_classes)\n",
    "train(net, num_epochs, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove redundant units. Notice that threshold \n",
    "# and hidden_size are parameters\n",
    "\n",
    "def remove_redundant_units(net, hidden_size, X, threshold=15):\n",
    "    hidden_units = net.hidden_layer(X).data.numpy()\n",
    "    weight2 = net.fc2.weight.data.numpy().copy()\n",
    "    bias = net.fc2.bias.data.numpy().copy()\n",
    "    removed = set()\n",
    "    num_removed = 0\n",
    "    \n",
    "    for i in range(hidden_size):\n",
    "        a = hidden_units[:, i].reshape(-1, 1)\n",
    "        if np.linalg.norm(a) < num_train * 0.01:\n",
    "            print('hidden unit: {0:2d}   norm: {1:.2f}'.format(i, np.linalg.norm(a)))\n",
    "            removed.add(i)\n",
    "            weight2[:, i].fill(0)\n",
    "            num_removed += 1\n",
    "    else:\n",
    "        print('There is no unit close to 0')\n",
    "\n",
    "    print('   units      angle')\n",
    "    for i in range(hidden_size):\n",
    "        if i in removed:\n",
    "            continue\n",
    "        a = hidden_units[:, i].reshape(-1, 1)\n",
    "        for j in range(i+1, hidden_size):\n",
    "            if j in removed:\n",
    "                continue\n",
    "            b = hidden_units[:, j].reshape(-1, 1)\n",
    "            cos = np.asscalar(a.T @ b / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "            cos = min(cos, 1)\n",
    "            cos = max(cos, -1)\n",
    "            angle = np.arccos(cos) * 180 / np.pi\n",
    "            if angle < threshold:\n",
    "                print('{0:4d} {1:4d}    {2:.4f}'.format(i, j, angle))\n",
    "                removed.add(j)\n",
    "                weight2[:, i] += weight2[:, j]\n",
    "                weight2[:, j].fill(0)\n",
    "                bias[:, i] += bias[:, j]\n",
    "                bias[:, j].fill(0)\n",
    "                num_removed += 1\n",
    "            if angle > 180 - threshold:\n",
    "                print('{0:4d} {1:4d}    {2:.4f}'.format(i, j, angle))\n",
    "                removed.add(j)\n",
    "                weight2[:, i] -= weight2[:, j]\n",
    "                weight2[:, j].fill(0)\n",
    "                bias[:, i] -= bias[:, j]\n",
    "                bias[:, j].fill(0)\n",
    "                num_removed += 1\n",
    "\n",
    "    return weight2, bias, num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.9738%\n",
      "Final test set accuracy: 95.8820%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  18   33    179.8629\n",
      "Number of removed units: 1\n",
      "After remove units, test set ccuracy: 95.8820%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.8954%\n",
      "Final test set accuracy: 95.8820%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   23    29.3004\n",
      "   5   28    153.0389\n",
      "  15   35    0.3220\n",
      "  15   39    0.0766\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.0490%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 50\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 99.9215%\n",
      "Final test set accuracy: 96.4942%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    4    138.0682\n",
      "   1   13    38.3025\n",
      "   1   32    38.3019\n",
      "   1   36    140.3568\n",
      "   3   11    140.4073\n",
      "   6   21    40.1346\n",
      "  12   14    136.1380\n",
      "  16   23    138.1565\n",
      "  31   34    144.7804\n",
      "  39   42    26.9032\n",
      "  43   44    143.7082\n",
      "Number of removed units: 11\n",
      "After remove units, test set ccuracy: 95.7151%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6055%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  28   34    175.8308\n",
      "  63   70    0.7412\n",
      "  63   75    1.3749\n",
      "  63   77    0.5908\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.6055%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  10   64    160.9522\n",
      "  24   39    157.5220\n",
      "  40   41    0.3000\n",
      "  40   63    179.9604\n",
      "  65   66    23.4861\n",
      "  67   74    0.5238\n",
      "  67   92    0.1454\n",
      "  67   98    180.0000\n",
      "Number of removed units: 8\n",
      "After remove units, test set ccuracy: 96.6055%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.3272%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   15    39.0029\n",
      "   2   20    162.9961\n",
      "   3   30    42.0288\n",
      "   4   24    42.2545\n",
      "   7   44    38.9579\n",
      "  11   23    44.7072\n",
      "  13   55    34.6338\n",
      "  14   43    36.0381\n",
      "  14   54    36.8684\n",
      "  14   74    146.3981\n",
      "  16   93    143.3076\n",
      "  18   19    148.1921\n",
      "  28   60    36.0127\n",
      "  32   33    135.9783\n",
      "  34   62    34.7971\n",
      "  36   38    0.0000\n",
      "  36   41    179.1377\n",
      "  47   58    0.3232\n",
      "  47   59    0.5585\n",
      "  47   71    179.6478\n",
      "  73   77    179.8813\n",
      "  82   96    158.7057\n",
      "  83   99    139.1754\n",
      "Number of removed units: 23\n",
      "After remove units, test set ccuracy: 96.2159%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 97.0506%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2    8    179.8493\n",
      "  20   34    166.2552\n",
      "  37   59    0.1998\n",
      "  37   71    174.9580\n",
      "  95  108    177.9988\n",
      "Number of removed units: 5\n",
      "After remove units, test set ccuracy: 97.0506%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.3829%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   3    6    180.0000\n",
      "  13   30    2.7611\n",
      "  13   31    172.1581\n",
      "  38   41    151.9781\n",
      "  39  125    28.0439\n",
      "  46   47    1.1975\n",
      "  46   49    1.9342\n",
      "  46   51    0.8891\n",
      "  46   77    179.4436\n",
      "  64  142    153.0453\n",
      "  80   99    2.2214\n",
      "  80  107    0.2974\n",
      "  80  111    0.4083\n",
      "  80  147    167.2133\n",
      "Number of removed units: 14\n",
      "After remove units, test set ccuracy: 96.4942%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 150\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.5498%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   1   24    44.1291\n",
      "   3   12    135.6986\n",
      "   5  137    44.4968\n",
      "   6  101    39.1243\n",
      "   7   47    41.9384\n",
      "   7   50    40.0161\n",
      "   7  107    32.9487\n",
      "   7  111    44.4459\n",
      "   7  132    138.5629\n",
      "  14   56    144.6747\n",
      "  15   98    42.5669\n",
      "  15  122    44.1028\n",
      "  15  145    136.3354\n",
      "  16   83    135.5933\n",
      "  17  102    40.9480\n",
      "  19  121    42.4240\n",
      "  20  147    146.4527\n",
      "  21   23    179.8533\n",
      "  22   84    144.9764\n",
      "  25   33    36.7581\n",
      "  25   38    138.0876\n",
      "  26  134    138.1670\n",
      "  27  106    135.7373\n",
      "  34   45    38.3165\n",
      "  35   97    136.3878\n",
      "  40   71    44.0639\n",
      "  41  120    44.2225\n",
      "  41  136    33.2484\n",
      "  42   44    179.8155\n",
      "  46  149    138.2966\n",
      "  48  119    39.1084\n",
      "  53   63    136.9385\n",
      "  55   66    40.4046\n",
      "  55   67    0.1009\n",
      "  55   86    0.3369\n",
      "  55   90    0.0000\n",
      "  55   91    2.4965\n",
      "  55   92    0.0000\n",
      "  55   95    167.8419\n",
      "  58  124    40.0734\n",
      "  59   64    144.0301\n",
      "  62   85    135.3166\n",
      "  68  142    43.9771\n",
      "  74   88    39.7895\n",
      "  74   99    33.7459\n",
      "  74  108    42.0321\n",
      "  76  140    40.5865\n",
      "  94  143    136.6823\n",
      "  96  100    43.9702\n",
      "  96  117    39.4386\n",
      "  96  131    39.0260\n",
      "  96  138    139.1518\n",
      " 104  114    35.7736\n",
      " 127  133    31.7429\n",
      "Number of removed units: 54\n",
      "After remove units, test set ccuracy: 96.0490%\n",
      "\n",
      "Threshold: 15\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.4942%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "  14   15    178.0142\n",
      "  89  108    176.8909\n",
      " 117  148    171.8921\n",
      " 165  176    179.5652\n",
      "Number of removed units: 4\n",
      "After remove units, test set ccuracy: 96.4942%\n",
      "\n",
      "Threshold: 30\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 97.1063%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   2   91    25.3780\n",
      "   4    6    0.8904\n",
      "   4   11    166.2393\n",
      "  16   26    1.0706\n",
      "  16   27    2.2084\n",
      "  16   39    179.9138\n",
      "  45   63    4.2181\n",
      "  45   78    160.3636\n",
      "  54   79    18.9745\n",
      "  74  125    26.3672\n",
      "  81   84    20.8893\n",
      "  81   88    0.2639\n",
      "  81   99    0.4681\n",
      "  81  128    0.4731\n",
      "  81  135    10.7237\n",
      "  81  144    153.1367\n",
      " 111  112    25.4200\n",
      " 111  160    22.9645\n",
      " 151  153    169.0926\n",
      " 171  172    179.6816\n",
      " 178  179    0.0198\n",
      " 178  185    179.7602\n",
      "Number of removed units: 22\n",
      "After remove units, test set ccuracy: 96.9950%\n",
      "\n",
      "Threshold: 45\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.5498%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0   89    43.8692\n",
      "   2   11    18.1645\n",
      "   2   12    157.4550\n",
      "   3  101    138.6187\n",
      "   4   28    41.9417\n",
      "   4   40    37.1756\n",
      "   4   72    137.0553\n",
      "   9   80    41.5541\n",
      "   9  172    39.1487\n",
      "   9  174    136.4114\n",
      "  15  151    41.9287\n",
      "  16  122    143.4428\n",
      "  18   34    137.6992\n",
      "  19  164    43.2978\n",
      "  20   38    136.7333\n",
      "  22  125    43.0155\n",
      "  22  131    135.0877\n",
      "  23   27    139.7544\n",
      "  30   52    6.2958\n",
      "  30   53    173.7058\n",
      "  33   43    39.0860\n",
      "  35  171    37.4313\n",
      "  36   39    43.8728\n",
      "  36  128    34.4097\n",
      "  36  157    135.0825\n",
      "  41   64    44.3683\n",
      "  41  108    153.0918\n",
      "  48  179    41.4409\n",
      "  49  156    135.3241\n",
      "  57   73    41.7315\n",
      "  58   61    168.9644\n",
      "  59  186    137.1747\n",
      "  66  147    43.5007\n",
      "  66  152    42.6015\n",
      "  66  176    138.1709\n",
      "  69   86    142.9045\n",
      "  75   78    13.3335\n",
      "  75   82    139.7450\n",
      "  83  110    41.9625\n",
      "  83  193    144.1583\n",
      "  84   90    138.4462\n",
      "  85   87    43.3945\n",
      "  92  135    41.0541\n",
      "  94  139    139.6749\n",
      "  98  149    138.1133\n",
      " 104  114    141.9572\n",
      " 111  191    40.9052\n",
      " 116  119    174.1863\n",
      " 117  120    156.0817\n",
      " 124  129    1.3763\n",
      " 124  132    175.6351\n",
      " 127  150    34.0595\n",
      " 127  188    44.9626\n",
      " 138  153    43.9941\n",
      " 138  170    136.0051\n",
      " 142  180    145.9209\n",
      " 145  168    42.0344\n",
      " 154  190    44.2295\n",
      " 163  198    142.8074\n",
      "Number of removed units: 59\n",
      "After remove units, test set ccuracy: 96.3272%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do some experiment\n",
    "\n",
    "num_epochs = 1000\n",
    "for hidden_size in [50, 100, 150, 200]:\n",
    "    net = Net(n_features, hidden_size, num_classes)\n",
    "    train(net, num_epochs, X, Y, if_plot=False)\n",
    "    for threshold in [15, 30, 45, 60, 75]:\n",
    "        print('Threshold:', threshold)\n",
    "        original_weight2 = net.fc2.weight.data.numpy()\n",
    "        weight2, bias, num_removed = remove_redundant_units(net, hidden_size, X, threshold=threshold)\n",
    "        print('Number of removed units:', num_removed)\n",
    "        net.fc2.weight.data = torch.from_numpy(weight2)\n",
    "        net.fc2.bias.data = torch.from_numpy(bias)\n",
    "        print('After remove units, test set ccuracy: {0:.4f}%'.format(test(net, X_test, Y_test)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 60\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.9393%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    3    133.4056\n",
      "   1    5    134.0296\n",
      "   2   37    121.5175\n",
      "   4   16    127.4017\n",
      "   6   19    128.5839\n",
      "   7   13    53.4266\n",
      "   7   15    126.5679\n",
      "   8   34    48.3320\n",
      "   8   42    51.6732\n",
      "   8   48    126.1983\n",
      "   9   31    55.8454\n",
      "   9   33    126.0322\n",
      "  10   21    120.1338\n",
      "  11   75    50.9888\n",
      "  11   93    124.6787\n",
      "  12   28    121.5131\n",
      "  14   22    130.8649\n",
      "  17   18    135.1267\n",
      "  20   26    179.7113\n",
      "  23   25    124.0251\n",
      "  24   82    126.8240\n",
      "  27   58    127.3123\n",
      "  30   55    122.8562\n",
      "  32   35    1.2529\n",
      "  32   43    34.3910\n",
      "  32   44    1.2532\n",
      "  32   45    1.2523\n",
      "  32   47    55.3486\n",
      "  32   50    36.8811\n",
      "  32   51    1.2686\n",
      "  32   52    178.7465\n",
      "  36   64    54.2343\n",
      "  36   68    52.7664\n",
      "  36   90    57.8454\n",
      "  38   49    48.9417\n",
      "  38   66    125.9545\n",
      "  39   54    55.1479\n",
      "  39   65    49.9118\n",
      "  39   74    121.2444\n",
      "  40   46    121.6037\n",
      "  41   77    124.2384\n",
      "  53   60    125.6078\n",
      "  56   61    45.7969\n",
      "  56   72    130.2282\n",
      "  57   71    130.9512\n",
      "  59   63    57.7507\n",
      "  59   83    58.3850\n",
      "  59   86    53.2555\n",
      "  59   94    54.3310\n",
      "  59   99    48.1788\n",
      "  62   73    123.8689\n",
      "  67   92    41.3025\n",
      "  69   78    120.0203\n",
      "  76   80    179.4822\n",
      "  79   98    125.1238\n",
      "  84   85    136.3374\n",
      "  87   89    49.1592\n",
      "  88   95    121.8789\n",
      "  91   96    134.4193\n",
      "Number of removed units: 59\n",
      "After remove units, test set ccuracy: 94.4352%\n",
      "\n",
      "Threshold: 80\n",
      "Starting training...\n",
      "Hidden units: 100\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    78.2077\n",
      "   0    2    69.7808\n",
      "   0    5    76.8759\n",
      "   0    6    76.4085\n",
      "   0    7    67.9247\n",
      "   0    8    70.1740\n",
      "   0    9    100.7167\n",
      "   3    4    101.5396\n",
      "  10   12    14.5673\n",
      "  10   16    76.1444\n",
      "  10   17    105.0092\n",
      "  11   14    111.7055\n",
      "  13   15    110.8620\n",
      "  18   22    112.3115\n",
      "  19   20    117.8926\n",
      "  21   23    112.3977\n",
      "  24   25    104.6142\n",
      "  26   30    108.0110\n",
      "  27   28    177.6554\n",
      "  29   31    117.0227\n",
      "  32   35    108.1169\n",
      "  33   34    110.9107\n",
      "  36   38    78.1554\n",
      "  36   39    112.4371\n",
      "  37   40    75.0375\n",
      "  37   41    114.8911\n",
      "  42   43    104.7660\n",
      "  44   45    63.7867\n",
      "  44   49    110.9521\n",
      "  46   47    77.0492\n",
      "  46   48    127.5370\n",
      "  50   51    111.6519\n",
      "  52   55    66.7132\n",
      "  52   59    103.7094\n",
      "  53   54    176.3569\n",
      "  56   62    102.6741\n",
      "  57   60    134.4297\n",
      "  58   65    101.0284\n",
      "  61   64    103.1845\n",
      "  63   67    76.5771\n",
      "  63   68    100.0026\n",
      "  66   69    128.8450\n",
      "  70   71    108.5168\n",
      "  72   73    69.7555\n",
      "  72   76    57.4129\n",
      "  72   77    75.5418\n",
      "  72   78    100.6834\n",
      "  74   75    58.8706\n",
      "  74   79    58.5408\n",
      "  74   80    76.8591\n",
      "  74   82    121.0118\n",
      "  81   83    104.1095\n",
      "  84   85    75.7031\n",
      "  84   87    120.8074\n",
      "  86   88    122.8057\n",
      "  89   90    75.3569\n",
      "  89   91    59.4356\n",
      "  89   92    78.7778\n",
      "  89   94    107.6431\n",
      "  93   97    122.8893\n",
      "  95   96    74.9716\n",
      "Number of removed units: 61\n",
      "After remove units, test set ccuracy: 89.2599%\n",
      "\n",
      "Threshold: 60\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    138.1146\n",
      "   2   22    57.5867\n",
      "   2   77    125.1831\n",
      "   3   39    120.3358\n",
      "   4   17    121.9770\n",
      "   5   42    121.4830\n",
      "   6   31    120.6770\n",
      "   7   51    58.5487\n",
      "   7   61    130.7127\n",
      "   8   10    129.1380\n",
      "   9   47    130.4159\n",
      "  11   14    125.8992\n",
      "  12   34    58.4027\n",
      "  12   92    134.8660\n",
      "  13   15    142.9059\n",
      "  16   19    125.1830\n",
      "  18   68    52.5150\n",
      "  18   71    51.3255\n",
      "  18  127    123.6532\n",
      "  20   28    122.7960\n",
      "  21   24    120.7606\n",
      "  23   56    58.7647\n",
      "  23   62    126.7026\n",
      "  25   27    126.6192\n",
      "  26   32    175.5658\n",
      "  29   82    58.8349\n",
      "  29   94    54.9490\n",
      "  29  147    45.8997\n",
      "  29  170    123.2338\n",
      "  30   45    130.3668\n",
      "  33   53    128.3610\n",
      "  35   46    152.8349\n",
      "  36   44    59.0787\n",
      "  36   48    123.9912\n",
      "  37   38    59.6789\n",
      "  37   57    120.9943\n",
      "  40   79    41.4055\n",
      "  40  118    125.0921\n",
      "  41   52    126.5853\n",
      "  43   59    120.4032\n",
      "  49   67    128.3780\n",
      "  50   60    143.0016\n",
      "  54   87    123.1491\n",
      "  55   74    57.3650\n",
      "  55   78    52.4364\n",
      "  55   84    59.9306\n",
      "  55   86    124.4307\n",
      "  58   99    127.1632\n",
      "  63   65    59.7653\n",
      "  63   90    53.4924\n",
      "  63   95    46.3313\n",
      "  63  103    54.8734\n",
      "  63  111    55.6503\n",
      "  63  129    123.3693\n",
      "  64   96    53.9835\n",
      "  64  110    56.4225\n",
      "  64  169    125.1405\n",
      "  66   72    130.7186\n",
      "  69  114    57.6185\n",
      "  69  115    55.5776\n",
      "  69  122    137.4940\n",
      "  70  124    54.3167\n",
      "  70  139    56.9259\n",
      "  70  159    133.5383\n",
      "  73   75    125.5683\n",
      "  76   80    59.7157\n",
      "  76   88    126.1741\n",
      "  81   98    130.6975\n",
      "  83  158    53.1548\n",
      "  85   89    40.2189\n",
      "  85   97    46.0608\n",
      "  85  101    41.7576\n",
      "  85  104    130.5072\n",
      "  93  121    123.4199\n",
      " 100  128    121.0288\n",
      " 102  109    130.4829\n",
      " 105  108    59.0632\n",
      " 105  156    122.7617\n",
      " 106  142    54.7321\n",
      " 106  155    57.6890\n",
      " 107  112    127.3868\n",
      " 113  116    49.4673\n",
      " 113  126    129.6925\n",
      " 117  119    122.1431\n",
      " 120  138    126.6748\n",
      " 123  130    58.9774\n",
      " 123  182    57.6210\n",
      " 123  198    57.8832\n",
      " 125  134    179.5860\n",
      " 131  132    54.3088\n",
      " 131  136    124.9397\n",
      " 133  152    122.8483\n",
      " 135  146    170.9675\n",
      " 137  140    128.2238\n",
      " 145  151    131.3472\n",
      " 148  160    125.3412\n",
      " 149  167    52.4788\n",
      " 149  176    120.0477\n",
      " 150  157    121.1214\n",
      " 153  162    120.4912\n",
      " 154  171    41.1761\n",
      " 154  172    131.3916\n",
      " 163  166    120.1349\n",
      " 164  165    128.8948\n",
      " 173  179    123.9968\n",
      " 174  191    45.3487\n",
      " 175  177    47.1806\n",
      " 175  184    128.3062\n",
      " 178  183    121.8161\n",
      " 187  188    27.0521\n",
      " 187  190    123.0069\n",
      " 192  194    122.0766\n",
      " 193  196    122.1769\n",
      "Number of removed units: 113\n",
      "After remove units, test set ccuracy: 95.8264%\n",
      "\n",
      "Threshold: 80\n",
      "Starting training...\n",
      "Hidden units: 200\n",
      "Number of epochs: 1000\n",
      "==============================\n",
      "Progress: ####################\n",
      "Final train set accuracy: 100.0000%\n",
      "Final test set accuracy: 96.6611%\n",
      "There is no unit close to 0\n",
      "   units      angle\n",
      "   0    1    104.7224\n",
      "   2    4    62.9582\n",
      "   2    6    112.9869\n",
      "   3    5    68.6474\n",
      "   3    7    102.7615\n",
      "   8   10    107.6774\n",
      "   9   11    69.4178\n",
      "   9   12    140.7066\n",
      "  13   14    135.1820\n",
      "  15   16    67.6254\n",
      "  15   18    73.9880\n",
      "  15   19    57.8047\n",
      "  15   20    76.8344\n",
      "  15   29    76.5586\n",
      "  15   30    102.4819\n",
      "  17   22    72.9239\n",
      "  17   23    58.0633\n",
      "  17   24    76.0168\n",
      "  17   25    70.9573\n",
      "  17   27    74.3784\n",
      "  17   33    120.4135\n",
      "  21   28    72.3168\n",
      "  21   35    76.8976\n",
      "  21   36    108.6133\n",
      "  26   31    78.8901\n",
      "  26   32    114.6994\n",
      "  34   38    74.1940\n",
      "  34   39    119.7559\n",
      "  37   40    101.9197\n",
      "  41   42    33.1530\n",
      "  41   44    109.4598\n",
      "  43   46    76.3043\n",
      "  43   47    104.4990\n",
      "  45   48    107.1389\n",
      "  49   50    74.4979\n",
      "  49   51    107.7807\n",
      "  52   53    107.8504\n",
      "  54   57    119.4889\n",
      "  55   56    75.5590\n",
      "  55   58    67.7626\n",
      "  55   59    73.1497\n",
      "  55   61    104.9490\n",
      "  60   63    47.9072\n",
      "  60   65    79.8199\n",
      "  60   66    112.4836\n",
      "  62   70    105.3123\n",
      "  64   68    63.0329\n",
      "  64   75    66.6435\n",
      "  64   79    79.8777\n",
      "  64   80    101.4112\n",
      "  67   69    65.7578\n",
      "  67   74    65.8998\n",
      "  67   77    78.3850\n",
      "  67   78    103.5854\n",
      "  71   76    37.4815\n",
      "  71   81    118.6966\n",
      "  72   73    120.2002\n",
      "  82   83    55.0238\n",
      "  82   85    102.0937\n",
      "  84   89    73.0983\n",
      "  84   95    109.5817\n",
      "  86   87    148.5716\n",
      "  88   90    103.2906\n",
      "  91   92    127.2069\n",
      "  93   94    73.3120\n",
      "  93   96    59.2210\n",
      "  93   97    79.7213\n",
      "  93  100    127.7782\n",
      "  98   99    111.3305\n",
      " 101  103    78.5325\n",
      " 101  104    79.4919\n",
      " 101  105    66.3201\n",
      " 101  107    104.8110\n",
      " 102  106    176.3400\n",
      " 108  110    105.6006\n",
      " 109  111    126.6232\n",
      " 112  114    110.9767\n",
      " 113  115    109.9322\n",
      " 116  119    107.0820\n",
      " 117  118    117.3960\n",
      " 120  123    64.6936\n",
      " 120  124    101.4916\n",
      " 121  126    78.7494\n",
      " 121  127    103.4730\n",
      " 122  125    102.8744\n",
      " 128  129    106.1636\n",
      " 130  133    104.7517\n",
      " 131  134    114.3407\n",
      " 132  135    63.2003\n",
      " 132  138    108.5463\n",
      " 136  140    107.7598\n",
      " 137  139    39.9699\n",
      " 137  143    102.4286\n",
      " 141  142    115.7755\n",
      " 144  148    103.0354\n",
      " 145  146    101.5185\n",
      " 147  149    67.1160\n",
      " 147  150    74.4683\n",
      " 147  151    75.1327\n",
      " 147  153    105.5293\n",
      " 152  157    103.0396\n",
      " 154  155    79.2521\n",
      " 154  158    77.3073\n",
      " 154  161    77.3425\n",
      " 154  162    109.7152\n",
      " 156  159    74.7900\n",
      " 156  160    59.7560\n",
      " 156  164    77.7373\n",
      " 156  165    79.9386\n",
      " 156  166    104.1027\n",
      " 163  168    78.8055\n",
      " 163  170    102.4073\n",
      " 167  171    109.9633\n",
      " 169  172    103.0527\n",
      " 173  174    101.6958\n",
      " 175  176    134.6502\n",
      " 177  178    100.5165\n",
      " 179  181    109.6372\n",
      " 180  182    70.7326\n",
      " 180  183    102.0263\n",
      " 184  185    106.0426\n",
      " 186  187    129.4376\n",
      " 188  189    61.9426\n",
      " 188  190    110.1568\n",
      " 191  192    103.4128\n",
      " 193  194    70.5793\n",
      " 193  195    101.6771\n",
      " 196  197    73.4081\n",
      " 196  199    100.5486\n",
      "Number of removed units: 129\n",
      "After remove units, test set ccuracy: 89.9833%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for hidden_size in [100,200]:\n",
    "    for threshold in [60, 80]:\n",
    "        print('Threshold:', threshold)\n",
    "        net = Net(n_features, hidden_size, num_classes)\n",
    "        train(net, num_epochs, X, Y, if_plot=False)\n",
    "        original_weight2 = net.fc2.weight.data.numpy()\n",
    "        weight2, num_removed = remove_redundant_units(net, hidden_size, X, threshold=threshold)\n",
    "        print('Number of removed units:', num_removed)\n",
    "        net.fc2.weight.data = torch.from_numpy(weight2)\n",
    "        print('After remove units, test set ccuracy: {0:.4f}%'.format(test(net, X_test, Y_test)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare this result with random remove units\n",
    "\n",
    "# hidden_size = 100\n",
    "# num_epochs = 1000\n",
    "# net = Net(n_features, hidden_size, num_classes)\n",
    "# train(net, num_epochs, X, Y, if_plot=False)\n",
    "# original_weight = net.fc2.weight.data.numpy()\n",
    "# random_num = np.random.randint(1, 100, 50)\n",
    "# print(random_num)\n",
    "# for i in random_num:\n",
    "#     original_weight2[:, i].fill(0)\n",
    "# net.fc2.weight.data = torch.from_numpy(original_weight2)\n",
    "# print('After remove units, test set ccuracy:', test(net, X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
