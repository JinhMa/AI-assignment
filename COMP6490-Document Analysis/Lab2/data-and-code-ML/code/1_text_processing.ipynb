{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Preprocessing for Machine Learning\n",
    "\n",
    "This tutorial will guide you through text preprocessing using NLTK, constructing a boolean representation for text and performing a boolean search as shown in the IR part of this course.\n",
    "These instructions assume that you have Python and the corresponding necessary package installed.\n",
    "If this is not the case, go back to the [installation instructions](https://wattlecourses.anu.edu.au/pluginfile.php/1412128/mod_resource/content/8/installation-and-usage-instructions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Compilation\n",
    "\n",
    "Download the [tutorial starting code from Wattle](https://wattlecourses.anu.edu.au/mod/resource/view.php?id=1198771) (~12MB), save it in a local folder and unzip it. \n",
    "**Warning:** This operation might take a couple of minutes, considering the number of files in the archive.\n",
    "The `source` folder contains Python code file and the tutorial notebooks.\n",
    "Navigate to this folder and start the jupyter notebook.\n",
    "\n",
    "If you have problems: use Google, ask a classmate who attended the lab for help or ask the tutor (in this order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the required libraries and making `jupyter notebook` configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some configurations for notebook and importing modules\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we use a subset of the [newsgroup dataset](https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html), entitled `two_newsgroup`, which has two  categories: `comp.graphics` and `sci.med`.\n",
    "\n",
    "We load the dataset into a [pandas Dataframe](https://pandas.pydata.org), which is a two-dimensional labeled data-structure (like a table).\n",
    "Columns represent attributes and rows represent data instances. \n",
    "In the `two_newsgroup` dataset, columns represent document id, category and text for each document (*i.e.* each row).\n",
    "\n",
    "The code for reading the dataset is provided in the python source file `data.py` (which the next block loads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns of the dataset: (1963, 3)\n",
      "The first five documents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59263</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59051</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59435</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: \"nigel allen\" &lt;nigel.allen@canrem.com&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59297</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: jtpoupor@undergrad.math.uwaterloo.ca (Je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59638</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>Subject: Re: cure for dry skin?\\nFrom: habersc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id category                                               text\n",
       "0  59263  sci.med  From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...\n",
       "1  59051  sci.med  From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...\n",
       "2  59435  sci.med  From: \"nigel allen\" <nigel.allen@canrem.com>\\n...\n",
       "3  59297  sci.med  From: jtpoupor@undergrad.math.uwaterloo.ca (Je...\n",
       "4  59638  sci.med  Subject: Re: cure for dry skin?\\nFrom: habersc..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import read_as_df\n",
    "## TODO start\n",
    "## specify the path to the dataset two_newsgroup\n",
    "path_to_dataset = '../data/two_newsgroups/'\n",
    "## TODO stop\n",
    "dataset = read_as_df(path_to_dataset)\n",
    "print(\"Number of rows and columns of the dataset: {}\".format(dataset.shape))\n",
    "print(\"The first five documents:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textual preprocessing\n",
    "\n",
    "As shown above, we have 1963 documents in our dataset. \n",
    "We extract and print the first document, which will be used throughout this section for introducing different text preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ttrusk@its.mcw.edu (Thomas Trusk)\n",
      "Subject: Re: Krillean Photography\n",
      "Organization: Medical College of Wisconsin\n",
      "Lines: 22\n",
      "Reply-To: ttrusk@its.mcw.edu\n",
      "NNTP-Posting-Host: pixel.cellbio.mcw.edu\n",
      "\n",
      "\n",
      "In article <20APR199315574161@vxcrna.cern.ch> filipe@vxcrna.cern.ch (VINCI) writes:\n",
      "\n",
      "> How about Kirlian imaging ? I believe the FAQ for sci.skeptics (sp?)\n",
      "> has a nice write-up on this. They would certainly be most supportive\n",
      "> on helping you to build such a device and connect to a 120Kvolt\n",
      "> supply so that you can take a serious look at your \"aura\"... :-)\n",
      ">\n",
      "> Filipe Santos\n",
      "> CERN - European Laboratory for Particle Physics\n",
      "> Switzerland\n",
      "\n",
      "Please sign the relevant documents and forward the remaining parts\n",
      "to our study 'Effect of 120 Kv on Human Tissue wrapped in Film'.\n",
      "Thanks for your support...\n",
      "*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*==*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n",
      "*Dr. Thomas Trusk                    *                              *\n",
      "*Dept. of Cellular Biology & Anatomy * Email to ttrusk@its.mcw.edu  *\n",
      "*Medical College of Wisconsin        *                              *\n",
      "*Milwaukee, WI  53226              DISCLAIMER (ala Foghorn Leghorn):*\n",
      "*(414) 257-8504                     It's a joke, son. A joke I say! *\n",
      "*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*==*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = dataset['text'][0]\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Machine Learning algorithms require the data to be represented as vectors, *i.e.* as array of numbers.\n",
    "We need to convert our document into a numeric vector. \n",
    "Before proceeding, we perform textual preprocessing and we convert the docuemnt to list of tokens. \n",
    "Some of the common preprocessing steps are:\n",
    "1. Tokenization \n",
    "2. Stop words removal\n",
    "3. Stemming\n",
    "\n",
    "We will cover these concepts one by one.\n",
    "\n",
    "### 2.1 Tokenization\n",
    "\n",
    "Tokenization divides a document into a sequence of tokens. Tokens are the basic unit of a document, *e.g.* words. \n",
    "The [NLTK](http://www.nltk.org) library provides different functionalities for text processing and natural language processing including tokenization, stemming and stop word removal. \n",
    "\n",
    "The following block defines a NLTK tokenizer based on a regex pattern which preserves abbreviations, words with hyphens, currencies and percentages, ellipsis (such as ...), *etc* when breaking a documents into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "__tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "## call it using tokenizer.tokenize\n",
    "tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct the corresponding list of tokens for an example phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', 'of', 'Australia', 'is', 'a', 'developed', ',', 'with', 'a', 'G.D.P.', 'of', 'AUD', '$1.67', 'trillion', 'as', 'of', '2016', 'and', 'a', 'growth', 'rate', 'of', '2.80%', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"\"\"\n",
    "The economy of Australia is a developed, with a G.D.P. of AUD$1.67 trillion as of 2016 and a growth rate of 2.80%.\n",
    "\n",
    "\"\"\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the tokenizer we just defined to tokenize our target document and obtain the corresponding list of tokens.\n",
    "Note that we are also converting the document to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens : 130\n",
      "tokens: ['from', ':', 'ttrusk', 'its', '.', 'mcw', '.', 'edu', '(', 'thomas', 'trusk', ')', 'subject', ':', 're', ':', 'krillean', 'photography', 'organization', ':', 'medical', 'college', 'of', 'wisconsin', 'lines', ':', '22', 'reply-to', ':', 'ttrusk', 'its', '.', 'mcw', '.', 'edu', 'nntp-posting-host', ':', 'pixel', '.', 'cellbio', '.', 'mcw', '.', 'edu', 'in', 'article', '20', 'apr199315574161', 'vxcrna', '.', 'cern', '.', 'ch', 'filipe', 'vxcrna', '.', 'cern', '.', 'ch', '(', 'vinci', ')', 'writes', ':', 'how', 'about', 'kirlian', 'imaging', '?', 'i', 'believe', 'the', 'faq', 'for', 'sci', '.', 'skeptics', '(', 'sp', '?', ')', 'has', 'a', 'nice', 'write-up', 'on', 'this', '.', 'they', 'would', 'certainly', 'be', 'most', 'supportive', 'on', 'helping', 'you', 'to', 'build', 'such', 'a', 'device', 'and', 'connect', 'to', 'a', '120', 'kvolt', 'supply', 'so', 'that', 'you', 'can', 'take', 'a', 'serious', 'look', 'at', 'your', '\"', 'aura', '\"', '...', ':', '-', ')', 'filipe', 'santos', 'cern', '-', 'european', 'laboratory', 'for', 'particle', 'physics', 'switzerland', 'please', 'sign', 'the', 'relevant', 'documents', 'and', 'forward', 'the', 'remaining', 'parts', 'to', 'our', 'study', \"'\", 'effect', 'of', '120', 'kv', 'on', 'human', 'tissue', 'wrapped', 'in', 'film', \"'\", '.', 'thanks', 'for', 'your', 'support', '...', 'dr', '.', 'thomas', 'trusk', 'dept', '.', 'of', 'cellular', 'biology', 'anatomy', 'email', 'to', 'ttrusk', 'its', '.', 'mcw', '.', 'edu', 'medical', 'college', 'of', 'wisconsin', 'milwaukee', ',', 'wi', '53226', 'disclaimer', '(', 'ala', 'foghorn', 'leghorn', ')', ':', '(', '414', ')', '257', '-', '8504', 'it', \"'\", 's', 'a', 'joke', ',', 'son', '.', 'a', 'joke', 'i', 'say']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(document.lower())\n",
    "print(\"number of tokens : {}\".format(len(set(tokens))))\n",
    "print(\"tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will filter out the non alphabetic tokens using the standard Python function [isalpha()](https://docs.python.org/2/library/stdtypes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens : 109\n",
      "tokens: ['from', 'ttrusk', 'its', 'mcw', 'edu', 'thomas', 'trusk', 'subject', 're', 'krillean', 'photography', 'organization', 'medical', 'college', 'of', 'wisconsin', 'lines', 'ttrusk', 'its', 'mcw', 'edu', 'pixel', 'cellbio', 'mcw', 'edu', 'in', 'article', 'vxcrna', 'cern', 'ch', 'filipe', 'vxcrna', 'cern', 'ch', 'vinci', 'writes', 'how', 'about', 'kirlian', 'imaging', 'i', 'believe', 'the', 'faq', 'for', 'sci', 'skeptics', 'sp', 'has', 'a', 'nice', 'on', 'this', 'they', 'would', 'certainly', 'be', 'most', 'supportive', 'on', 'helping', 'you', 'to', 'build', 'such', 'a', 'device', 'and', 'connect', 'to', 'a', 'kvolt', 'supply', 'so', 'that', 'you', 'can', 'take', 'a', 'serious', 'look', 'at', 'your', 'aura', 'filipe', 'santos', 'cern', 'european', 'laboratory', 'for', 'particle', 'physics', 'switzerland', 'please', 'sign', 'the', 'relevant', 'documents', 'and', 'forward', 'the', 'remaining', 'parts', 'to', 'our', 'study', 'effect', 'of', 'kv', 'on', 'human', 'tissue', 'wrapped', 'in', 'film', 'thanks', 'for', 'your', 'support', 'dr', 'thomas', 'trusk', 'dept', 'of', 'cellular', 'biology', 'anatomy', 'email', 'to', 'ttrusk', 'its', 'mcw', 'edu', 'medical', 'college', 'of', 'wisconsin', 'milwaukee', 'wi', 'disclaimer', 'ala', 'foghorn', 'leghorn', 'it', 's', 'a', 'joke', 'son', 'a', 'joke', 'i', 'say']\n"
     ]
    }
   ],
   "source": [
    "## filter out non alphabets\n",
    "alphabet_tokens = [token for token in tokens if token.isalpha()]\n",
    "print(\"number of tokens : {}\".format(len(set(alphabet_tokens))))\n",
    "print(\"tokens: {}\".format(alphabet_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stopwords removal\n",
    "Stopwords are commonly used words that do not provide any document specific information, such as **the**, **from**, **to**, **at** *etc*.\n",
    "We use the NLTK standard stopwords list to remove stopwords from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens : 80\n",
      "tokens: ['ttrusk', 'mcw', 'edu', 'thomas', 'trusk', 'subject', 'krillean', 'photography', 'organization', 'medical', 'college', 'wisconsin', 'lines', 'ttrusk', 'mcw', 'edu', 'pixel', 'cellbio', 'mcw', 'edu', 'article', 'vxcrna', 'cern', 'ch', 'filipe', 'vxcrna', 'cern', 'ch', 'vinci', 'writes', 'kirlian', 'imaging', 'believe', 'faq', 'sci', 'skeptics', 'sp', 'nice', 'would', 'certainly', 'supportive', 'helping', 'build', 'device', 'connect', 'kvolt', 'supply', 'take', 'serious', 'look', 'aura', 'filipe', 'santos', 'cern', 'european', 'laboratory', 'particle', 'physics', 'switzerland', 'please', 'sign', 'relevant', 'documents', 'forward', 'remaining', 'parts', 'study', 'effect', 'kv', 'human', 'tissue', 'wrapped', 'film', 'thanks', 'support', 'dr', 'thomas', 'trusk', 'dept', 'cellular', 'biology', 'anatomy', 'email', 'ttrusk', 'mcw', 'edu', 'medical', 'college', 'wisconsin', 'milwaukee', 'wi', 'disclaimer', 'ala', 'foghorn', 'leghorn', 'joke', 'son', 'joke', 'say']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "non_stopwords = [word for word in alphabet_tokens if not word in en_stopwords]\n",
    "print(\"number of tokens : {}\".format(len(set(non_stopwords))))\n",
    "print(\"tokens: {}\".format(non_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is the process of reducing inflected words to their word stem, base or root form. \n",
    "The NLTK python library also contains an implementation of the [Snowball stemming](http://snowballstem.org) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens : 79\n",
      "tokens: ['ttrusk', 'mcw', 'edu', 'thoma', 'trusk', 'subject', 'krillean', 'photographi', 'organ', 'medic', 'colleg', 'wisconsin', 'line', 'ttrusk', 'mcw', 'edu', 'pixel', 'cellbio', 'mcw', 'edu', 'articl', 'vxcrna', 'cern', 'ch', 'filip', 'vxcrna', 'cern', 'ch', 'vinci', 'write', 'kirlian', 'imag', 'believ', 'faq', 'sci', 'skeptic', 'sp', 'nice', 'would', 'certain', 'support', 'help', 'build', 'devic', 'connect', 'kvolt', 'suppli', 'take', 'serious', 'look', 'aura', 'filip', 'santo', 'cern', 'european', 'laboratori', 'particl', 'physic', 'switzerland', 'pleas', 'sign', 'relev', 'document', 'forward', 'remain', 'part', 'studi', 'effect', 'kv', 'human', 'tissu', 'wrap', 'film', 'thank', 'support', 'dr', 'thoma', 'trusk', 'dept', 'cellular', 'biolog', 'anatomi', 'email', 'ttrusk', 'mcw', 'edu', 'medic', 'colleg', 'wisconsin', 'milwauke', 'wi', 'disclaim', 'ala', 'foghorn', 'leghorn', 'joke', 'son', 'joke', 'say']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stems = [str(stemmer.stem(word)) for word in non_stopwords]\n",
    "print(\"number of tokens : {}\".format(len(set(stems))))\n",
    "print(\"tokens: {}\".format(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of unique tokens decreases with each of the preprocessing steps. Why did this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation task\n",
    "Your current task is to **build the a preprocessing function** which takes an entire document and performs all of the above preprocessing steps (*i.e.* converting to lowercase, tokenization, removing stopwords and stemming) in a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    '''\n",
    "        turns text into tokens after tokenization, stemming, stop words removal\n",
    "        imput:\n",
    "            - text: document to process\n",
    "        output: =>\n",
    "            - tokens: list of tokens after tokenization, stemming, stop words removal\n",
    "    '''\n",
    "    stems = []\n",
    "    \n",
    "    ## TODO: fill each of the steps below\n",
    "    ## Step 1: tokenization\n",
    "    ## Step 2: remove non-alphabetic characters\n",
    "    ## Step 3: remove stopwords\n",
    "    ## Step 4: stemm tokens\n",
    "\n",
    "    ## TODO end\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use your newly implmented function to create a column of `tokens` in our dataframe. \n",
    "The method [`DataFrame.apply`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) takes a function and applies it to each row of the dataframe. \n",
    "\n",
    "**note**: the next block might a bit to execute depending on the efficiency of your implementation and on the performances of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59263</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59051</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59435</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: \"nigel allen\" &lt;nigel.allen@canrem.com&gt;\\n...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59297</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: jtpoupor@undergrad.math.uwaterloo.ca (Je...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59638</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>Subject: Re: cure for dry skin?\\nFrom: habersc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id category                                               text tokens\n",
       "0  59263  sci.med  From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...     []\n",
       "1  59051  sci.med  From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...     []\n",
       "2  59435  sci.med  From: \"nigel allen\" <nigel.allen@canrem.com>\\n...     []\n",
       "3  59297  sci.med  From: jtpoupor@undergrad.math.uwaterloo.ca (Je...     []\n",
       "4  59638  sci.med  Subject: Re: cure for dry skin?\\nFrom: habersc...     []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['tokens'] = dataset['text'].apply(preprocessor)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistics over textual tokens\n",
    "We compute the histogram of tokens across the entire dataset and we search for the 10 most frequent tokens with their counts.\n",
    "We also produce the plot of the frequency of the most common 50 tokens (the X axis presents tokens, ordered by their frequency).\n",
    "\n",
    "You can change 50 to a higher value to have even more tokens plotted.\n",
    "What do you observe about the obtained distribution of token frequency? Does it look like a **rich-get-richer** effect?\n",
    "What does this mean in the context of textual processing?\n",
    "What other quantities present such a distribution (think about the distribution of heights vs. the distribution of income in a population of humans)?\n",
    "\n",
    "Notice the python function [itertools.chain](https://docs.python.org/2/library/itertools.html#itertools.chain), of which you can think as a function that concatenates a list of lists into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4oAAAFmCAYAAAArlGQoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11accf160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "fdist = nltk.FreqDist(chain(*dataset['tokens']))\n",
    "print(fdist.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(15, 6))  # the size you want\n",
    "fdist.plot(50,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique tokens do we have in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "## total number of unique tokens in the collection\n",
    "all_tokens = set(chain(*dataset['tokens']))\n",
    "print(\"number of tokens in the dataset: {}\".format(len(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Boolean representation and search\n",
    "### 4.1 Boolean representation\n",
    "The sparse boolean representation represents a document as a set of words. \n",
    "Therefore, we create a new column in our dataset to represent each document as **a set** of tokens (as opposed to **a vector** of tokens in the previous column).\n",
    "One of the advantages of the set representation is that it allows accesing efficient python methods for searching is a token exists in a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59263</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59051</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59435</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: \"nigel allen\" &lt;nigel.allen@canrem.com&gt;\\n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59297</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>From: jtpoupor@undergrad.math.uwaterloo.ca (Je...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59638</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>Subject: Re: cure for dry skin?\\nFrom: habersc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id category                                               text tokens  \\\n",
       "0  59263  sci.med  From: ttrusk@its.mcw.edu (Thomas Trusk)\\nSubje...     []   \n",
       "1  59051  sci.med  From: nyeda@cnsvax.uwec.edu (David Nye)\\nSubje...     []   \n",
       "2  59435  sci.med  From: \"nigel allen\" <nigel.allen@canrem.com>\\n...     []   \n",
       "3  59297  sci.med  From: jtpoupor@undergrad.math.uwaterloo.ca (Je...     []   \n",
       "4  59638  sci.med  Subject: Re: cure for dry skin?\\nFrom: habersc...     []   \n",
       "\n",
       "  features  \n",
       "0       {}  \n",
       "1       {}  \n",
       "2       {}  \n",
       "3       {}  \n",
       "4       {}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['features'] = dataset['tokens'].apply(set)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Boolean Search\n",
    "\n",
    "We implement a simple boolean search through the document collection. \n",
    "The boolean search was presented in the IR lectures (refresh your memory using the [link to the corresponding IR lecture slides](https://wattlecourses.anu.edu.au/pluginfile.php/1481743/mod_resource/content/0/lecture1_intro_to_br.pdf#page=13) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with searching for documents which contain the words `research` or `seminar`.\n",
    "\n",
    "To do this, we create in the following block a predicate function that returns true if the document `doc` contains `research` or `seminar` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_matches = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, category, text, tokens, features]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predicate(doc): \n",
    "    return stemmer.stem('research') in doc or stemmer.stem('seminar') in doc\n",
    "\n",
    "## an array of true or false indicating match or ot match at corresponnding index\n",
    "matches = dataset['features'].apply(predicate)\n",
    "print('num_matches = {}'.format(dataset[matches].size))\n",
    "dataset[matches].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we stem each term in the query in the predicate. Why did we do this?\n",
    "\n",
    "## Implementation task\n",
    "Now search for documents containing `silicon` and `valley`. What should the predicate function be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-17-4e6cda05ff65>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-4e6cda05ff65>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    matches = dataset['features'].apply(predicate)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def predicate(doc): \n",
    "    ## TODO: implement the predicate function\n",
    "    \n",
    "    ## TODO stop\n",
    "\n",
    "matches = dataset['features'].apply(predicate)\n",
    "print('num_matches = {}'.format(dataset[matches].size))\n",
    "dataset[matches].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
