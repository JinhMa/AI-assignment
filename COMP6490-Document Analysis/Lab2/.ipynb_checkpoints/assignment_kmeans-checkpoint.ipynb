{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering using KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to code up K-means clustering as discussed in lecture and apply it to the [*Assignment 2 Q1* Data posted to Wattle](https://wattlecourses.anu.edu.au/mod/resource/view.php?id=1130230).\n",
    "Parts of the code and extensive instructions are provided in the rest of this notebook.\n",
    "Read them carefully in order to receive full marks.\n",
    "Note that there are four implementation tasks: one is to implement KMeans, and the other three are at the end of the notebook.\n",
    "\n",
    "During the lab, you will be asked to run your code on a *new* data set, similar to the *Assignment 2 Q1 Data*, but containing different documents.\n",
    "Consider this as a **train/test split**: during the development of your assignment, you observe the **training set**.\n",
    "During the lab, you are evaluated on the **testing set**.\n",
    "As you will notice below, the code displays the *top 5 document filenames* for each of the K clusters found. \n",
    "The tutor will both inspect the quality (purity and the homogeneity measure) of the clustering (2.5 pts), as well as the *efficiency* of the code you write and your explanation of your design choices (2.5 pts).\n",
    "\n",
    "**Notes:**  \n",
    "\n",
    "* If your algorithm randomizes its initialization, we are allowed to run it multiple times and we will grade the best clustering found.\n",
    "* The starting code uses the preprocessing function that we have developed during the tutorials. Feel free to experiment with other techniques to improve performance (ask Google, read the lectures etc.) We will give additional marks to such experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some configurations for notebook and importing modules\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading dataset and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and we preprocess it in a similar manner to the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import read_as_df\n",
    "## TODO start\n",
    "## provide path to the dataset\n",
    "path_to_dataset = '../../data/assign_data_train/'\n",
    "## TODO stop\n",
    "\n",
    "dataset = read_as_df(path_to_dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing function developed in the tutorials, and present in the file `prepros.py`. \n",
    "If you decide to make any improvements to the pre-processing, make it in a block in this notebook, as the tutors don't have access to any external files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepros import preprocessor\n",
    "\n",
    "dataset['tokens'] = dataset['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton for kmeans algorithm is given in the next block. You have to implement/complete the following functions. The details of the functions, input, output are defined here after, or in the comments of each function.:\n",
    "\n",
    "1. **init_centroids**: Initializes the centroids (you can experiment here with random init, kmeans++ init etc).\n",
    "2. **cost**: returns the total distance between documents and the centroid for each cluster\n",
    "3. **reassign**: returns the new document labels (after the centroids have been updated)\n",
    "4. **recompute**: returns the new centroids (after the labels have been updated)\n",
    "5. **fit**: The algorithm that iteratively updates labels and centroids until convergence\n",
    "6. **get_n_documents**: return the index of top n (e.g. top 5) documents in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist import dist, search\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin\n",
    "import scipy\n",
    "\n",
    "class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):\n",
    "    '''\n",
    "        Custom implementation of KMeans that fits to sklearn's pipeline\n",
    "    '''\n",
    "    \n",
    "    ## Don't modify this constructor\n",
    "    def __init__(self, K, max_iter = 100, distance = 'cosine', tol = 1e-3):\n",
    "        '''\n",
    "            constructor for KMeans class\n",
    "\n",
    "            arguments:\n",
    "                - K: number of clusters\n",
    "                - max_iter: max number of iterations\n",
    "                - distance: distace measure to use - 'cosine' or 'euclid'\n",
    "                - tol: tolerance for convergence\n",
    "        '''\n",
    "\n",
    "        assert K >= 1, ('invalid K , must be +ve')\n",
    "        assert max_iter >= 1, ('invalid max_iter, must be +ve')\n",
    "        assert tol > 0 and tol < 1, ('tol must be in rangd (0, 1)')\n",
    "\n",
    "        self.K = K\n",
    "        self.max_iters = max_iter\n",
    "        self.dist = distance\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.tolerance = tol\n",
    "\n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def init_centroids(self, X):\n",
    "        '''\n",
    "            this method returns the initial centroids\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - centroids: centroids matrix (k * d dims)\n",
    "        '''\n",
    "        \n",
    "        n, d = X.shape\n",
    "        centroids = np.zeros((self.K, d))\n",
    "        \n",
    "        ## TODO start\n",
    "        ## INSTRUCTION: randomly choose self.K rows from X\n",
    "        ## and assign them as centroids and return centroids\n",
    "        ## Hint: look at np.random.choice function\n",
    "\n",
    "        ## TODO stop\n",
    "        \n",
    "        return centroids\n",
    "    \n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def cost(self, X):\n",
    "        '''\n",
    "            this method returns the sum of distance between centroid and points for each cluster\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - dists: vector of length k where dists[i] is sum of distance\n",
    "                            between centroid[i] and points that lie in cluster i\n",
    "        '''\n",
    "        costs = np.zeros(self.K)\n",
    "        for k in range(self.K):\n",
    "            ## TODO start\n",
    "            ## instructions: compute the sum of the distances between documents belonging to cluster k and its centroid\n",
    "            ## hint: you might want to use dist function from dist.py here\n",
    "\n",
    "            ## TODO end\n",
    "        return costs\n",
    "    \n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def reassign(self, X):\n",
    "        '''\n",
    "            this method returns the new labels for each data point in X\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - labels: vector of length n where labels[i] is the\n",
    "                    cluster label for ith point\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        new_assign = np.zeros(n)\n",
    "        ## TODO start\n",
    "        ## instructions: compute the new assignments for each row in X\n",
    "        ## hint: you might want to use dist function from dist.py here\n",
    "\n",
    "        ## TODO end\n",
    "        return new_assign\n",
    "    \n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def recompute(self, X):\n",
    "        '''\n",
    "            this method returns new centroids\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - centroids: new centroids computed from labels\n",
    "        '''\n",
    "        ## TODO\n",
    "        n, d = X.shape\n",
    "        centroids = np.zeros((self.K, d))\n",
    "        ## TODO start\n",
    "        ## instructions: compute the new centroids\n",
    "\n",
    "        ## TODO end\n",
    "        return centroids\n",
    "    \n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def fit(self, X, y = None):\n",
    "        '''\n",
    "            this method is the body of the KMeans algorithm. It regroups the data X into k clusters\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - self\n",
    "        '''\n",
    "        if type(X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.toarray()\n",
    "\n",
    "        n_iter = 0\n",
    "        converged = False\n",
    "        ## TODO start\n",
    "        ## Step 1: initialize centroids and labels\n",
    "\n",
    "        ## TODO stop\n",
    "\n",
    "        ## iterate\n",
    "        while n_iter < self.max_iters and not converged:\n",
    "            ## TODO start\n",
    "            ## Step 2: recompute centroids, recompute new document assignation, check stopping criteria\n",
    "            \n",
    "            ## TODO end\n",
    "            obj = self.cost(X).sum()\n",
    "            print('iter = {}, objective = {}'.format(n_iter, obj))\n",
    "            n_iter += 1\n",
    "\n",
    "        return self\n",
    "    \n",
    "    ## TODO: Look inside this function for instructions\n",
    "    def get_n_documents(self, X, n = 5):\n",
    "        '''\n",
    "            this method returns the index of top n documents from each cluster\n",
    "            input =>\n",
    "                - X input data matrix (n * d dims)\n",
    "                - n: number of top documents to return\n",
    "            output: =>\n",
    "                - results: list of tuple (k, index) which means doc at index belongs to cluster k\n",
    "        '''\n",
    "        if type(X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.toarray()\n",
    "        labels = self.transform(X)\n",
    "        results = []\n",
    "        for k in range(self.K):\n",
    "            ## TODO: return either names of indexes of the top 5 documents in each cluster\n",
    "            ## Hint: look inside dist.py for help on this\n",
    "\n",
    "            ## TODO stop\n",
    "        return results\n",
    "\n",
    "    ## all the methods below are required for the integration with scikit-learn.\n",
    "    ## DO NOT EDIT ANY METHOD BELOW HERE\n",
    "    def transform(self, X, y = None):\n",
    "        '''\n",
    "            this method returns the labels by inferencing on fitted model\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - labels: inferred models\n",
    "        '''\n",
    "        if type(X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.toarray()\n",
    "        return self.reassign(X)\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        '''\n",
    "            this method returns the labels by fitting X to the model\n",
    "            input =>\n",
    "                - X : data matrix (n * d dims)\n",
    "            output: =>\n",
    "                - labels: inferred models\n",
    "        '''\n",
    "        self.fit(X)\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Kmeans class that you have just implemented and we create a `scikit-learn pipeline` which performs clustering with the  TF-IDF document representation and the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## encoding dataset labels (refer classifier tutorial)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dataset.category)\n",
    "\n",
    "## bag of words vectorizer (refer classifier tutorial)\n",
    "bow_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                     tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                     stop_words = None, ## stop words removal already done from NLTK\n",
    "                                     max_features = 5000, ## pick top 5K words by frequency\n",
    "                                     ngram_range = (1, 1), ## we want unigrams now\n",
    "                                     binary = False) ## Now it's Bag of Words\n",
    "\n",
    "## build a pipeline\n",
    "pipeline_cosine = Pipeline([\n",
    "    ('bow',  bow_vectorizer),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('k-means',  KMeans(K = len(list(le.classes_)), distance = 'cosine', tol = 1e-4) ) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training KMeans\n",
    "We can now perform training by calling `pipeline.fit` function and get the predictions by calling `pipeline.transform` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_cosine.fit(dataset.tokens)\n",
    "preds_cosine = pipeline_cosine.transform(dataset.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get top 5 documents from each cluster\n",
    "\n",
    "Now using the function *get_n_documents*, find and print the top 5 documents from each cluster. We can get individual component of pipeline using function `pipeline.named_steps['k-means']`.\n",
    "\n",
    "Before using this function, we need to access the vectorial representation of the dataset. \n",
    "Since we used a pipeline to do both the document vectorization and the clustering together, we will repeat the process without the last step (i.e. the clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = Pipeline(pipeline_cosine.steps[:-1]) ## all components of pipeline except last component, which is kmeans\n",
    "vectors = vectorizer.transform(dataset.tokens)\n",
    "top_5_idxs = pipeline_cosine.named_steps['k-means'].get_n_documents(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the top 5 documents in each cluster (the 5 documents closest to the centroid, in each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k, idx) in top_5_idxs:\n",
    "    print(\"({}, {}) belongs to cluster {}\".format(dataset.iloc[idx].category, dataset.iloc[idx].id, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does a cluster contains only documents from a single document category? \n",
    "Ideally, documents from the same category should belong to the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluating cluster performance\n",
    "There are several cluster evaluating metrics readily available in `sklearn.metrics.cluster`, including the `homogeneity_score`, which is similar to the purity measure covered in the lecture.\n",
    "A partition resulted from clustering obtains a homogeneity score of 1 if all of its clusters contain only data points which are members of a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "print(\"homogenity = {}\".format(\n",
    "    homogeneity_score(y, preds_cosine)) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Additional assignment tasks\n",
    "\n",
    "This section contains three additional tasks. \n",
    "Note that for each of the tasks, you are required to solve each of the three steps here below, each in its own cell:\n",
    "1. construct a processing pipeline (as shown in the tutorials) which solves the task, \n",
    "2. compute and print the homogeneity score obtained using the pipeline\n",
    "3. plot the barplot of this score against the score obtained by the implementation in Section 2\n",
    "\n",
    "### 3.1 Task 2: Compare the KMeans clustering when using Unigrams vs. Bigrams as features.\n",
    "The KMeans clustering that we tested in Section 2 used Unigrams as representation features.\n",
    "Implement a processing pipeline which uses Bigrams as features and compare the performance gain (or loss) w.r.t. Unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: implement BIGRAMS processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: compute homogeneity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: barplot performances of BIGRAMS (this task) vs. UNIGRAMS (computed in Section 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Task 3: Cosine vs Euclidean distance.\n",
    "The algorithm in Section 2 used the Cosine distance to measure similarity between document (and/or centroids).\n",
    "In this task, you will measure the performance gain/loss when using the Euclidean distance instead.\n",
    "\n",
    "**Hint:** use the adequate argument in K-Means constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: implement KMEANS with Euclidean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: compute homogeneity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: barplot performances of EUCLIDEAN (this task) vs. COSINE (computed in Section 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Task 4: compare against the K-Means implementation in `scikit-learn`\n",
    "As you might have surely doubted, a mature library such as `scikit-learn` contains a readily implementation of KMeans.\n",
    "In this task, you will compare our homegrown KMeans implementation with the `scikit-learn`'s KMeans.\n",
    "\n",
    "**Hint** you might want to look at [this for reference](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: use KMeans from scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: compute homogeneity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: barplot performances of scikit-learn KMeans (this task) vs. our own (computed in Section 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Play with the parameters of the `scikit-learn` KMeans function and improve its performances. What do you need to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: improve performances of scikit-learn KMeans\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
