{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhaolongfei/Documents/ANU/COMP6490-Document Analysis/Lab/Lab4_nlp_assignment\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys, getopt\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "curr_path = os.path.abspath('__file__')\n",
    "root_path = os.path.abspath(os.path.join(curr_path, os.pardir))\n",
    "print(root_path)\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from models.FastText import FastText\n",
    "\n",
    "Dataset = namedtuple('Dataset','sentences labels')\n",
    "\n",
    "num_classes = 3\n",
    "learning_rate = 0.05\n",
    "num_epochs = 2\n",
    "embedding_dim = 10\n",
    "label_to_id = {'World':0, 'Entertainment':1, 'Sports':2}\n",
    "unknown_word_id = 0\n",
    "\n",
    "\n",
    "def create_label_vec(label):\n",
    "    label_id = label_to_id[label.strip()]\n",
    "    label_vec = [0] * num_classes\n",
    "    label_vec[label_id] = 1\n",
    "    return label_vec\n",
    "\n",
    "\n",
    "def tokenize(sens):\n",
    "    return word_tokenize(sens)\n",
    "\n",
    "def map_token_seq_to_word_id_seq(token_seq, word_to_id):\n",
    "    return [map_word_to_id(word_to_id,word) for word in token_seq]\n",
    "\n",
    "\n",
    "def map_word_to_id(word_to_id, word):\n",
    "    if word in word_to_id:\n",
    "        return word_to_id[word]\n",
    "    else:\n",
    "        return unknown_word_id\n",
    "\n",
    "\n",
    "def build_vocab(sens_file_name):\n",
    "    data = []\n",
    "    with open(sens_file_name) as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = tokenize(line)\n",
    "            data.extend(tokens)\n",
    "    print('size of token sequence is %s. ' % len(data))\n",
    "    count = [['$UNK$', 0]]\n",
    "    sorted_counts = collections.Counter(data).most_common()\n",
    "    count.extend(sorted_counts)\n",
    "    word_to_id = dict()\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    print(\"Unknown word id is %s .\" % word_to_id['$UNK$'])\n",
    "    print('size of vocabulary is %s. ' % len(word_to_id))\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def read_labeled_dataset(sens_file_name, label_file_name, word_to_id):\n",
    "    sens_file = open(sens_file_name)\n",
    "    label_file = open(label_file_name)\n",
    "    data = []\n",
    "    for label in label_file:\n",
    "        sens = sens_file.readline()\n",
    "        word_id_seq = map_token_seq_to_word_id_seq(tokenize(sens), word_to_id)\n",
    "        data.append((word_id_seq, create_label_vec(label)))\n",
    "    print(\"read %d sentences from %s .\" % (len(data), sens_file_name))\n",
    "    sens_file.close()\n",
    "    label_file.close()\n",
    "    return data\n",
    "\n",
    "def read_dataset(sens_file_name, word_to_id):\n",
    "    sens_file = open(sens_file_name)\n",
    "    data = []\n",
    "    for sens in sens_file:\n",
    "        word_id_seq = map_token_seq_to_word_id_seq(tokenize(sens), word_to_id)\n",
    "        data.append(word_id_seq)\n",
    "    print(\"read %d sentences from %s .\" % (len(data), sens_file_name))\n",
    "    sens_file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def eval(word_to_id, train_dataset, dev_dataset, test_dataset):\n",
    "    fast_text = FastText(num_classes, embedding_dim, len(word_to_id), learning_rate)\n",
    "    fast_text.build_graph()\n",
    "    init = tf.global_variables_initializer()\n",
    "    test_results = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffle(train_dataset)\n",
    "            for sens in train_dataset:\n",
    "                fast_text.train_step.run(feed_dict={fast_text.input_sens: sens[0], fast_text.correct_label: sens[1]})\n",
    "            print('Epoch %d : %s .' % (epoch, compute_accuracy(fast_text, dev_dataset)))\n",
    "\n",
    "\n",
    "        print('Accuracy on the test set : %s.' % compute_accuracy(fast_text, test_dataset))\n",
    "        test_results = predict(fast_text, test_dataset)\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def compute_accuracy(fast_text, eval_dataset):\n",
    "    num_correct = 0\n",
    "    for (sens, label) in eval_dataset:\n",
    "        num_correct += fast_text.accuracy.eval(feed_dict={fast_text.input_sens: sens, fast_text.correct_label: label})\n",
    "    print('#correct sentences is %s ' % num_correct)\n",
    "    return num_correct / len(eval_dataset)\n",
    "\n",
    "\n",
    "def predict(fast_text, test_dataset):\n",
    "    test_results = []\n",
    "    for (sens, label) in test_dataset:\n",
    "        test_results.append(fast_text.predict.eval(feed_dict={fast_text.input_sens: sens}))\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def write_result_file(test_results, result_file):\n",
    "    with open(result_file, mode='w') as f:\n",
    "         for r in test_results:\n",
    "             f.write(\"%d\\n\" % r)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    trainSensFile = ''\n",
    "    trainLabelFile = ''\n",
    "    devSensFile = ''\n",
    "    devLabelFile = ''\n",
    "    testSensFile = ''\n",
    "    testLabelFile = ''\n",
    "    testResultFile = ''\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hd:\",[\"dataFolder=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print('fastText.py -d <dataFolder>')\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print('fastText.py -d <dataFolder>')\n",
    "            sys.exit()\n",
    "        elif opt in (\"-d\", \"--dataFolder\"):\n",
    "            trainSensFile = os.path.join(arg, 'sentences_train.txt')\n",
    "            devSensFile = os.path.join(arg, 'sentences_dev.txt')\n",
    "            testSensFile = os.path.join(arg, 'sentences_test.txt')\n",
    "            trainLabelFile = os.path.join(arg, 'labels_train.txt')\n",
    "            devLabelFile = os.path.join(arg, 'labels_dev.txt')\n",
    "            testLabelFile = os.path.join(arg, 'labels_test.txt')\n",
    "            testResultFile = os.path.join(arg, 'test_results_2.txt')\n",
    "        else:\n",
    "            print(\"unknown option %s .\" % opt)\n",
    "    word_to_id = build_vocab(trainSensFile)\n",
    "    train_dataset = read_labeled_dataset(trainSensFile, trainLabelFile, word_to_id)\n",
    "    dev_dataset = read_labeled_dataset(devSensFile, devLabelFile, word_to_id)\n",
    "    test_dataset = read_labeled_dataset(testSensFile, testLabelFile, word_to_id)\n",
    "    test_results = eval(word_to_id, train_dataset, dev_dataset, test_dataset)\n",
    "    write_result_file(test_results, testResultFile)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#   main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of token sequence is 1215500. \n",
      "Unknown word id is 0 .\n",
      "size of vocabulary is 56493. \n",
      "read 150158 sentences from data/sentences_train.txt .\n",
      "read 21451 sentences from data/sentences_dev.txt .\n",
      "read 42902 sentences from data/sentences_test.txt .\n",
      "#correct sentences is [ 8159.  8159.  8159.] \n",
      "Epoch 0 : [ 0.38035524  0.38035524  0.38035524] .\n",
      "#correct sentences is [ 8159.  8159.  8159.] \n",
      "Epoch 1 : [ 0.38035524  0.38035524  0.38035524] .\n",
      "#correct sentences is [ 16381.  16381.  16381.] \n",
      "Accuracy on the test set : [ 0.38182369  0.38182369  0.38182369].\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c7fe7f4dc73b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-d'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-dd8ee0b338e5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mdev_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_labeled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevSensFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevLabelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_labeled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSensFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mwrite_result_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestResultFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-dd8ee0b338e5>\u001b[0m in \u001b[0;36meval\u001b[0;34m(word_to_id, train_dataset, dev_dataset, test_dataset)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy on the test set : %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-dd8ee0b338e5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(fast_text, test_dataset)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfast_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "main(['-d','data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(3, 5, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
