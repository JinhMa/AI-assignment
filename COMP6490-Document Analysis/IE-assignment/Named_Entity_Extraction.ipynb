{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is a slight modification of the tutorial by Sam Galen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from features import *\n",
    "from evaluate import *\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the CoNLL 2002 data to build a Named Entity Recognition (NER) system\n",
    "\n",
    "The CoNLL2002 corpus is available in NLTK. We use the Spanish (esp.) data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format: Lets see how our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Define some features to characterize the Named Entities. Find a feature extraction python code example in [features](./features.py). In the example, the features are: word identity, word suffix, word shape and word POS-tag.\n",
    "\n",
    "This makes a simple baseline.  You can add and/or remove features to get (much?) better results - experiment with it as you will need to do this for assignment.\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what word2features extracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1:postag': 'Fpa',\n",
       " '+1:postag[:2]': 'Fp',\n",
       " '+1:word.lower()': '(',\n",
       " 'BOS': True,\n",
       " 'bias': 1.0,\n",
       " 'postag': 'NP',\n",
       " 'postag[:2]': 'NP',\n",
       " 'word.lower()': 'melbourne',\n",
       " 'word[-2:]': 'ne',\n",
       " 'word[-3:]': 'rne'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NER model\n",
    "\n",
    "To train a NER model: (1) invoque the pycrfsuite.Trainer method, (2) load the training data, (3) set the CRF training parameters, (4) call the \"trainer\" method to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training parameters. We will use L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 10.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible parameters for the default training algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and name the resulting model file (e.g.: conll2002-esp.crfsuite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 99.9 ms, total: 15.9 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.train saves model to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 zhaolongfei  staff   116K Oct 23 12:48 ./ner-esp.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./ner-esp.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get information for every training step using trainer.logparser.iterations\n",
    "\n",
    "Here we are extracting the information about the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'num': 50, 'scores': {}, 'loss': 48787.828124, 'feature_norm': 35.995718, 'error_norm': 1800.820649, 'active_features': 2306, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.268}\n"
     ]
    }
   ],
   "source": [
    "print (len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use your NER model, you need to create pycrfsuite.Tagger, open the model, and use the \"tag\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f705beb9790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tag a sentence to see how it works: (1) print the first sentence of the test set, (2) use your tagger to make predictions in that sentence, (3) print the predicted labels, (4) print the correct labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Coru√±a , 23 may ( EFECOM ) .\n",
      "\n",
      "Predicted: B-LOC I-LOC O O O O B-ORG O O\n",
      "Correct:   B-LOC I-LOC O O O O B-ORG O O\n"
     ]
    }
   ],
   "source": [
    "example_sent = test_sents[0]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the NER model\n",
    "\n",
    "The evaluation code is in the Python file named [evaluate](evaluate.py). Please don't change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, get the predicted entity labels for all sentences in the test set ('testb' Spanish data set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 376 ms, sys: 4 ms, total: 380 ms\n",
      "Wall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, see how good are you doing by running the custom evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.47      0.55      1084\n",
      "      I-LOC       0.52      0.25      0.34       325\n",
      "     B-MISC       0.54      0.11      0.19       339\n",
      "     I-MISC       0.54      0.22      0.32       557\n",
      "      B-ORG       0.76      0.51      0.61      1400\n",
      "      I-ORG       0.67      0.44      0.53      1104\n",
      "      B-PER       0.73      0.68      0.71       735\n",
      "      I-PER       0.78      0.82      0.80       634\n",
      "\n",
      "avg / total       0.68      0.48      0.55      6178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what the classifier has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-PER  -> I-PER   5.034204\n",
      "B-MISC -> I-MISC  4.509289\n",
      "B-ORG  -> I-ORG   4.445884\n",
      "I-MISC -> I-MISC  4.169687\n",
      "I-ORG  -> I-ORG   4.148371\n",
      "I-PER  -> I-PER   3.584838\n",
      "B-LOC  -> I-LOC   3.583397\n",
      "I-LOC  -> I-LOC   3.565042\n",
      "O      -> O       2.874174\n",
      "O      -> B-ORG   1.657239\n",
      "O      -> B-MISC  1.099429\n",
      "O      -> B-LOC   1.004697\n",
      "O      -> B-PER   0.827228\n",
      "B-ORG  -> O       0.678863\n",
      "B-PER  -> O       0.015656\n",
      "\n",
      "Top unlikely transitions:\n",
      "I-PER  -> B-PER   -1.178009\n",
      "B-LOC  -> I-MISC  -1.179662\n",
      "I-ORG  -> B-LOC   -1.199538\n",
      "I-ORG  -> I-PER   -1.305765\n",
      "I-PER  -> I-LOC   -1.314171\n",
      "B-ORG  -> I-MISC  -1.416927\n",
      "I-MISC -> I-ORG   -1.484186\n",
      "I-ORG  -> I-MISC  -1.530820\n",
      "B-PER  -> B-PER   -1.690435\n",
      "I-MISC -> I-LOC   -1.801041\n",
      "I-ORG  -> I-LOC   -1.970001\n",
      "O      -> I-PER   -4.536760\n",
      "O      -> I-MISC  -4.856690\n",
      "O      -> I-ORG   -5.194917\n",
      "O      -> I-LOC   -5.523952\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
    "\n",
    "Check the state features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "4.569708 O      postag[:2]:Fp\n",
      "4.139291 B-MISC word.lower():juegos\n",
      "3.839936 B-ORG  word.lower():efe\n",
      "3.248121 B-ORG  word.lower():psoe-progresistas\n",
      "3.133776 B-ORG  word.lower():telef√≥nica\n",
      "3.120209 O      -1:word.lower():efe\n",
      "2.994530 B-LOC  -1:word.lower():nuboso\n",
      "2.938371 B-LOC  word.lower():l√≠bano\n",
      "2.810946 B-PER  -1:word.lower():dijo\n",
      "2.796526 B-ORG  word.lower():amena\n",
      "2.778038 B-ORG  word[-2:]:OE\n",
      "2.734662 I-LOC  -1:word.lower():calle\n",
      "2.669057 B-ORG  word.lower():ej√©rcito\n",
      "2.633509 B-PER  -1:word.lower():seg√∫n\n",
      "2.629499 B-PER  word.lower():reyes\n",
      "2.597705 B-MISC word[-3:]:Ley\n",
      "2.583170 B-LOC  -1:word.lower():despejado\n",
      "2.571595 I-ORG  -1:word.lower():asociaci√≥n\n",
      "2.531081 I-LOC  -1:word.lower():san\n",
      "2.509848 B-LOC  word.lower():c√°ceres\n",
      "\n",
      "Top negative:\n",
      "-1.421753 B-PER  word[-2:]:os\n",
      "-1.432009 B-ORG  word[-2:]:or\n",
      "-1.506883 B-ORG  word[-2:]:de\n",
      "-1.527245 O      word[-3:]:Los\n",
      "-1.626755 B-LOC  word[-3:]:la\n",
      "-1.681866 O      word[-3:]:opa\n",
      "-1.683306 O      word.lower():estados\n",
      "-1.684262 O      word[-2:]:UE\n",
      "-1.718929 O      word[-3:]:Las\n",
      "-1.763166 I-LOC  BOS\n",
      "-1.775617 O      word.lower():mas\n",
      "-1.810057 O      word.lower():ayuntamiento\n",
      "-1.839557 O      word[-2:]:a.\n",
      "-1.853964 O      word.lower():universidad\n",
      "-2.001986 O      word.lower():congreso\n",
      "-2.013054 B-LOC  word[-3:]:i√≥n\n",
      "-2.023953 O      word.lower():parlamento\n",
      "-2.112873 O      postag:NP\n",
      "-2.112873 O      postag[:2]:NP\n",
      "-2.739515 B-PER  -1:word.lower():del\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "* **3.248121 B-ORG  word.lower=psoe-progresistas** - the model remembered names of some entities - maybe it is overfit, or maybe our features are not adequate, or maybe remembering is indeed helpful;\n",
    "* **2.734662 I-LOC  -1:word.lower=calle**: \"calle\" is a street in Spanish; model learns that if a previous word was \"calle\" then the token is likely a part of location;\n",
    "* **-2.112873 O      postag=NP** - proper nouns (NP is a proper noun in the Spanish tagset) are often entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
