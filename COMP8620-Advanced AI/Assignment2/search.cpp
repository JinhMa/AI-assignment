#include "search.hpp"

#include "agent.hpp"
#include <cmath>
#include <cassert>
#include "util.hpp"
typedef unsigned long long visits_t;

enum node_type_t { CHANCE_NODE, DECISION_NODE };


// search options
static const visits_t     MinVisitsBeforeExpansion = 1;
static const unsigned int MaxDistanceFromRoot  = 100;
static size_t             MaxSearchNodes;
#define MaxChildNodes 20

static const double exploration_constant = 2.0;


// Playout is rollout step which is a simulation by Content Tree
// Each node the first visit will be generated by rollout

static reward_t playout(Agent &agent, unsigned int playout_len) {
    
    // TODO: implement
    reward_t reward = 0.0;
    while (playout_len-- > 0) {
        // Execute an action chosen uniformly at random.
        action_t a = agent.genRandomAction();
        agent.modelUpdate(a);
        
        // Sample a percept.
        percept_t o,r;
        agent.genPerceptAndUpdate(o,r);
        reward += r;
    }
    
    return reward;
    
    
}


// contains information about a single "state"
class SearchNode {
    
    
private:
    
    bool m_chance_node; // true if this node is a chance node, false otherwise
    double m_mean;      // the expected reward of this node
    visits_t m_visits;  // number of times the search node has been visited
    
    // TODO: decide how to reference child nodes
    //  e.g. a fixed-size array
    
    //Use a fixed-size array to store the pointer of child node
    //max child size can be modified by predetermine value
    SearchNode * m_child[MaxChildNodes];
public:
    
    
    SearchNode(bool is_chance_node){
        m_mean = 0;
        m_visits = 0;
        m_chance_node = is_chance_node;
        for(int i=0; i<MaxChildNodes;i++ )
            m_child[i] = NULL;
        
    }
    
    SearchNode * get_child(const int child_index) const {
        return m_child[child_index] == NULL ? NULL : m_child[child_index];
    }
    bool exist_child(const int child_index) const {
        return m_child[child_index] == NULL ? false : true;
    }
    
    // in this step it will expand a decision node to make an action
    action_t selectAction(Agent &agent) {// TODO: implement
        
       
        const double unexplored_priority = INFINITY; //near to infinity
    
        
        
        action_t best_action;
        double best_priority = -INFINITY;//near to -infinity
        for (action_t a = 0; a < agent.numActions(); a++) {
            SearchNode *n = get_child(a);
            
            //UCB
            double priority = 0.0;
            if (n == NULL || n->visits() < MinVisitsBeforeExpansion) {
                priority = unexplored_priority;  // Unexplored node will have a highest priority to next visit
            } else {                             // Currently is a visited node, compute UCB value
                priority = n->expectation() + std::sqrt(exploration_constant * std::log((double) visits()) / (n->visits()));
            }

            if (priority > best_priority) {
                best_action = a;
                best_priority = priority;
            }
        }
        
        return best_action;
    }
    
    reward_t expansion(Agent &agent, unsigned int & horizon){
        percept_t o,r;
        reward_t reward = 0.0;
        agent.genPerceptAndUpdate(o,r);
        if (exist_child(o) == false)
            m_child[o] = new SearchNode( DECISION_NODE);
        reward = r + m_child[o]->sample(agent, horizon - 1);
        return reward;
    }
    // determine the expected reward from this node
    reward_t expectation(void) const { return m_mean; }
    
    // perform a sample run through this node and it's children,
    // returning the accumulated reward from this sample run
    reward_t sample(Agent &agent, unsigned int horizon)  {
        reward_t reward = 0.0;
        if(horizon == 0){
            return 0;
        }
        else if(m_chance_node == CHANCE_NODE)
        {
            reward = expansion(agent,horizon);
        }
        else if( m_visits < MinVisitsBeforeExpansion)
        {
            reward = playout(agent, horizon);
        }
        else
        {
            action_t a = selectAction(agent);
            agent.modelUpdate(a);
            if (!exist_child(a))
                m_child[a] = new SearchNode( CHANCE_NODE );
            reward = m_child[a]->sample(agent, horizon);
        }
        // Update the expected reward and number of visits to the current node.
        m_mean = (reward + visits() * expectation()) / (visits() + 1.0);
        m_visits++;
        return reward;
    }
    // number of times the search node has been visited
    visits_t visits(void) const { return m_visits; }
    
    
};





// determine the best action by searching ahead using MCTS
extern action_t search(Agent &agent) {
    /** The root node of the UCT search tree. */
    int simulation_times = agent.getSimulationTimes()?  agent.getSimulationTimes() : 300;
    //horizon
    size_t horizon = agent.horizon();
    assert(horizon <= MaxDistanceFromRoot );
    
    //Save agent status
    ModelUndo undo = ModelUndo(agent);
    
    SearchNode * mcts =  new SearchNode(DECISION_NODE);
    
    for(int i = 0; i < simulation_times; i++){
        mcts->sample(agent, horizon);
        //Reverse agent state for next search
        agent.modelRevert(undo);
    }

    action_t best_action = agent.genRandomAction();
    double best_mean = -INFINITY;
    for (action_t a = 0;  a < agent.numActions(); a++) {
        if (! mcts->exist_child(a))
            continue;
        //get mean ucb value
        double mean = mcts->get_child(a)->expectation();
        if (mean > best_mean) {
            best_mean = mean;
            best_action = a;
        }
    }
    delete mcts;
    return best_action;
    
}