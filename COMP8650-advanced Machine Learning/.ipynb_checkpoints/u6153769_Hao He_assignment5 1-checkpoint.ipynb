{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 \n",
    "### u6153769, Hao He\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**$\\bullet$ Linear Programming.** Write a Python script using cvx to solve the following linear over $x \\in \\mathbb{R}^4$: \n",
    "\n",
    "\\begin{align*}\n",
    "\\rm minimize~ & x_1 + 2x_2 + 3x_3 + 4x_4 \\\\\n",
    "\\rm subject~to~ & x_1 + x_2 + x_3 + x_4 = 1 \\\\\n",
    "            & x_1 - x_2 + x_3 - x_4 = 0 \\\\\n",
    "            & x_1, x_2, x_3, x_4 \\geq 0 \\\\\n",
    "\\end{align*} \n",
    "Include a printout of the optimal value ($x^{\\star}$ and $p^{\\star}$) and the source code in your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import cvxpo\n",
    "# Problem data.\n",
    "dim = 4 # dim denotes the dimension, i.e., R^dim, e.g., dim=4 => R^4\n",
    "x = cvx.Variable(dim)\n",
    "a = np.array([[1, 2, 3, 4]])\n",
    "a1 = np.ones((1,4))\n",
    "a2 = np.array([[1, -1, 1, -1]])\n",
    "\n",
    "# Create three constraints.\n",
    "constraints = [a1 * x == 1,\n",
    "               a2 * x == 0,\n",
    "               x >= 0]\n",
    "# Form objective.\n",
    "objective = cvx.Minimize(a*x)\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cvx.Problem(objective, constraints)\n",
    "prob.solve()  # Returns the optimal value.\n",
    "print(\"status:\", prob.status)\n",
    "print(\"optimal value p:\", prob.value)\n",
    "print(\"optimal var x:\", x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\bullet$ Regularized Maximum Likelihood Estimation.** In this problem we will develop a convex optimization problem for learning the parameters of a Gaussian distribution using samples generated from the distribution.\n",
    "\n",
    "* **(a)** Let $\\mathcal{D} = \\{x_1, \\dots ,x_m\\}$; be a set of samples drawn from a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$. Set $\\mu$ to the empirical mean (i.e., $ \\mu = \\frac{1}{m}\\sum_{i=1}^m x_i $). Show that the average log-likelihood of the data can be written as \n",
    "$$ \\ell(\\mathcal{D}) = \\rm log~det~\\Sigma^{-1} - tr(\\hat{\\Sigma} \\Sigma^{-1}) + const $$ \n",
    "where $ \\hat{\\Sigma} = \\frac{1}{m}\\sum_{i=1}^m (x_i-\\mu)(x_i-\\mu)^T $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Solution.**     \n",
    "Suppose $x \\in \\mathbb{R}^n$, the density of a Gaussian distribution with the mean $\\mu$ and covariance matrix $\\Sigma$ is\n",
    "$$ p_{\\mu, \\Sigma}(x)= (2\\pi)^{-\\frac{n}{2}} {\\rm det}(\\Sigma)^{-\\frac{1}{2}}{\\rm exp}(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu)) $$\n",
    "The log-likelihood function has the form   \n",
    "$$ \\begin{align*}\n",
    "l(\\Sigma) \n",
    "&= {\\rm log}~p_{\\mu, \\Sigma}(x_1, \\dots, x_m) \\\\\n",
    "&= {\\rm log}~\\prod_{i=1}^m p_{\\mu, \\Sigma}(x_i) \\\\\n",
    "&= \\sum_{i=1}^m {\\rm log}~((2\\pi)^{-\\frac{n}{2}} {\\rm det}(\\Sigma)^{-\\frac{1}{2}}{\\rm exp}(-\\frac{1}{2}(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu))) \\\\\n",
    "&= {-\\frac{n}{2}} \\sum_{i=1}^m {\\rm log}~(2\\pi) {-\\frac{1}{2}}\\sum_{i=1}^m{\\rm log~det}~\\Sigma -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu)) \\\\\n",
    "&= {-\\frac{mn}{2}} {\\rm log}~(2\\pi) {-\\frac{m}{2}}{\\rm log~det}~\\Sigma -\\frac{1}{2} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu)) \\\\\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average log-likelihood function has the form\n",
    "$$ \\begin{align*}\n",
    "{\\rm E}l(\\Sigma)\n",
    "&= {-\\frac{n}{2}} {\\rm log}~(2\\pi) {-\\frac{1}{2}}{\\rm log ~det}~\\Sigma -\\frac{1}{2m} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu)) \\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "$ \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu)) = {\\rm a~constant} = {\\rm tr}(\\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu))$,      \n",
    "$ \\because \\hat{\\Sigma} = \\frac{1}{m}\\sum_{i=1}^m (x_i-\\mu)(x_i-\\mu)^T $ and the trace of a product of matrices is independent of the order of the multiplication, i.e., ${\\rm tr}[ABC\\dots] = {\\rm tr}[BC\\dots A] = {\\rm tr}[C\\dots AB] = \\dots $     \n",
    "$ \\therefore \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^T \\Sigma^{-1}(x_i - \\mu)) = {\\rm tr}(\\hat{\\Sigma} \\Sigma^{-1})$  \n",
    "$$ \\begin{align*}\n",
    "{\\rm E}l(\\Sigma) \n",
    "&= {-\\frac{n}{2}} {\\rm log}~(2\\pi) {-\\frac{1}{2}}{\\rm log~det}~\\Sigma -\\frac{1}{2} {\\rm tr}(\\hat{\\Sigma} \\Sigma^{-1})\\\\\n",
    "&={\\frac{1}{2}} (-n{\\rm log}~(2\\pi) + {\\rm log~det}~\\Sigma^{-1} - {\\rm tr}(\\hat{\\Sigma} \\Sigma^{-1}))\n",
    "\\end{align*} $$\n",
    "\n",
    "The average log-likelihood of the data\n",
    "$$ \\begin{align*}\n",
    "\\ell(\\mathcal{D}) \n",
    "&= {\\rm E}l(\\Sigma) \\\\\n",
    "&\\propto \\rm log~det~\\Sigma^{-1} - tr(\\hat{\\Sigma} \\Sigma^{-1}) + const \n",
    "\\end{align*} $$\n",
    "Therefore, the average log-likelihood of the data can be written as \n",
    "$$ \\ell(\\mathcal{D}) = \\rm log~det~\\Sigma^{-1} - tr(\\hat{\\Sigma} \\Sigma^{-1}) + const $$ \n",
    "which is an equivalent problem of ${\\rm E}l(\\Sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** Suppose we wish to learn a sparse representation of the Gaussian. (There are a number of\n",
    "reasons why we may wish to do this). We can induce sparsity by placing an $\\ell_1$-penalty on the off-diagonal terms in the inverse covariance matrix. The resulting regularized maximum likelihood optimization problem can be expressed as   \n",
    "\\begin{align*}\n",
    "\\rm minimize~& {\\rm -log~det}~K + {\\rm tr}(\\hat{\\Sigma}K) + \\lambda \\Sigma_{i \\neq j} |K_{ij}| \\\\\n",
    "\\rm subject~to~& K \\succ 0\n",
    "\\end{align*}\n",
    "where $K = \\Sigma^{-1}$. Show that this is a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Solution.**      \n",
    "1). For the term ${\\rm log~det}~K$, we have the proof in Log-determinant of 3.1.5 Examples in *Convex Optimization*, p74.     \n",
    "For the function $f(X) = {\\rm log~det}~X $, we can verify concavity by considering an arbitrary line, given by $X = Z + tV$, where $Z, V \\in \\mathbf{S}^n$. We define $g(t) = f(Z + tV)$, and restrict $g$ to the interval of values of $t$ for which $Z +tV \\succ 0$. Without loss of generality, we can assume that $t = 0$ is inside this interval, i.e.,\n",
    "$Z \\succ 0$. We have \n",
    "$$ \\begin{align*}\n",
    "g(t) \n",
    "&= {\\rm log~det}(Z + tV) \\\\\n",
    "&= {\\rm log~det}(Z^{\\frac{1}{2}}(I + tZ^{-\\frac{1}{2}}V Z^{-\\frac{1}{2}})Z^{\\frac{1}{2}}) \\\\\n",
    "&= \\sum_{i=1}^n {\\rm log}(1 + tλ_i) + {\\rm log~det}~Z\n",
    "\\end{align*} $$\n",
    "where $λ_1,\\dots, λ_n$ are the eigenvalues of $Z^{-\\frac{1}{2}}V Z^{-\\frac{1}{2}})Z^{\\frac{1}{2}}$. Therefore we have\n",
    "$$\n",
    "g′(t) = \\sum_{i=1}^n \\frac{λ_i}{1 + tλ_i},~~ g′′(t) = -\\sum_{i=1}^n \\frac{λ_i^2}{(1 + tλ_i)^2}.\n",
    "$$\n",
    "Since $g''(t) ≤ 0$, we conclude that $f$ is concave.      \n",
    "So, $-{\\rm log~det}~K = -f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). For the term ${\\rm tr}(\\hat{\\Sigma}K)$, we have the similar proof in example 3.4 Matrix fractional function of 3.1.7 Epigraph in *Convex Optimization*, p76.     \n",
    "$ {\\rm tr}(\\hat{\\Sigma}K) = {\\rm tr}(\\hat{\\Sigma}\\Sigma^{-1}) = \\frac{1}{m} \\sum_{i=1}^m (x - \\mu)^T \\Sigma^{-1}(x - \\mu) $     \n",
    "Let $z = x - \\mu$, which is an affine function preserving the convexity. The function $f : \\mathbb{R}^n \\times \\mathbf{S}^n \\to \\mathbb{R}$, defined as\n",
    "$$ f = z^T \\Sigma^{-1}z $$\n",
    "is convex on $ {\\rm dom}~f = \\mathbb{R}^n \\times \\mathbf{S}^n_{++} $.    \n",
    "The convexity of $f$ can be established via its epigraph:\n",
    "$$ \\begin{align*}\n",
    "{\\rm epi}~f\n",
    "&= \\{(z, \\Sigma, t)~|~\\Sigma \\succ 0,~z^T \\Sigma^{-1}z \\le t \\} \\\\\n",
    "&= \\left\\{(z, \\Sigma, t)~\\bigg|\n",
    "  \\left[\\begin{matrix}\n",
    "   \\Sigma & z  \\\\\n",
    "   z^T & t  \\\\\n",
    "  \\end{matrix}\\right]\\succeq 0,~\\Sigma \\succ 0\n",
    "\\right\\},\n",
    "\\end{align*}$$     \n",
    "because $\\Sigma \\succ 0,~z^T \\Sigma^{-1}z \\le t \\} \\Longleftrightarrow \n",
    "\\left[\\begin{matrix}\n",
    "   \\Sigma & z  \\\\\n",
    "   z^T & t  \\\\\n",
    "  \\end{matrix}\\right]\\succeq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Sigma, \\Sigma^{-1} \\in \\mathbf{S}^n_{++}$, and $ {\\rm det}~\\Sigma \\ne 0$.\n",
    "\n",
    "Let \n",
    "$ W = \\begin{bmatrix}\n",
    "   \\Sigma & z  \\\\\n",
    "   z^T & t  \\\\\n",
    "  \\end{bmatrix}$, \n",
    "the Schur complement of $\\Sigma$ in $W$ is $ S = t − z^T \\Sigma^{−1} z \\ge 0$.      \n",
    "\n",
    "If $\\Sigma ≻ 0$, then $W \\succeq 0$ if and only if $S \\succeq 0$.\n",
    "\n",
    "$W \\succeq 0$ is a linear matrix inequality (LMI) in $(z, \\Sigma, t)$ in general form, we can write it in standard form as \n",
    "$ t \\begin{bmatrix} \n",
    "0&0 \\\\\n",
    "0&1\\\\ \\end{bmatrix} + \n",
    "\\Sigma \\begin{bmatrix} \n",
    "1&0 \\\\\n",
    "0&0\\\\ \\end{bmatrix} +\n",
    "z^T \\begin{bmatrix} \n",
    "0&0 \\\\\n",
    "1&0\\\\ \\end{bmatrix} +\n",
    "z \\begin{bmatrix} \n",
    "0&0 \\\\\n",
    "0&1\\\\ \\end{bmatrix} \\succeq 0 $.\n",
    "\n",
    "The solution set $ \\left\\{(z, \\Sigma, t)~\\bigg|\n",
    "  \\left[\\begin{matrix}\n",
    "   \\Sigma & z  \\\\\n",
    "   z^T & t  \\\\\n",
    "  \\end{matrix}\\right]\\succeq 0,~\\Sigma \\succ 0\n",
    "\\right\\}$ of the linear matrix inequality in $(z, \\Sigma, t)$,  is convex, and therefore ${\\rm epi}~f$ is convex.\n",
    "\n",
    "The sum of convex functions of $f$ is convex and the affine function of the sum is also convex. Therefore ${\\rm tr}(\\hat{\\Sigma}K)$ is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms. 3.1.5 p73-Eng   \n",
    "3). The 3rd term $\\lambda \\Sigma_{i \\neq j} |K_{ij}|$ is a linear combination of the sum of the norm $|K_{ij}|$. According to Norms of 3.1.5 Examples in *Convex Optimization*, p73, If $f : \\mathbb{R}^n \\to \\mathbb{R}$ is a norm, and $0 \\le \\theta \\le 1$, then  \n",
    "$$ f(\\theta x + (1 − \\theta)y) \\le f(\\theta x) + f((1 − \\theta)y) = \\theta f(x) + (1 − \\theta)f(y). $$\n",
    "The inequality follows from the triangle inequality, and the equality follows from homogeneity of a norm.      \n",
    "So, the norms are convex, and the 3rd term is convex.\n",
    "\n",
    "Therefore, the problem (b) is a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(c)** Using the data provided in [asgn5q2.pkl](https://wattlecourses.anu.edu.au/pluginfile.php/1761006/mod_resource/content/4/asgn5data.zip) write cvx code to solve the above regularized optimization problem. You can load the data and compute the empirical covariance matrix $\\hat{\\Sigma}$ using\n",
    "\n",
    "> \\# Python     \n",
    "> import cvxpy as cvx     \n",
    "> import numpy as np     \n",
    "> import pickle     \n",
    "> X = pickle.load(open('asgn5q2.pkl', 'rb'))     \n",
    "> m, n = X.shape     \n",
    "> sigmaHat = np.cov(X, rowvar=0)\n",
    "\n",
    "Plot the optimal objective value, log-likelihood (without regularization term), and number of\n",
    "non-zeros in the inverse covariance matrix as a function of $\\lambda$. Include source code with your\n",
    "solutions.\n",
    "\n",
    "*Hints.* 1. Use **A = cvx.Variable((n, n), PSD=True)** as a variable declaration to indicate that\n",
    "A is a positive semidefinite matrix. 2. Use the function **log_det** for ${\\rm log~det}(A)$. 3. When checking\n",
    "for sparsity truncate all entries in the inverse covariance with magnitude less than $10^{-6}$ to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Problem data and parameters.\n",
    "X = pickle.load(open('asgn5q2.pkl', 'rb'))\n",
    "m, n = X.shape # m = 100, n = 10\n",
    "sigmaHat = np.cov(X, rowvar=0) # 10x10 each column represents a variable, while the rows contain observations.\n",
    "\n",
    "K = cvx.Variable((n,n), PSD=True)\n",
    "lambd = cvx.Parameter(nonneg=True)\n",
    "TRIALS = 100\n",
    "lambda_vals = np.logspace(-4, 1, TRIALS)\n",
    "\n",
    "optimal_objective_vals =[]\n",
    "log_likelihood_vals = []\n",
    "number_non_zeros_list = [] \n",
    "\n",
    "# L1-penalty \n",
    "regularizer = lambd * cvx.trace((1 - np.eye(n)) * cvx.abs(K)) # ⟨𝑿,𝒀⟩=𝐭𝐫(𝑿.𝑻*𝒀)\n",
    "# regularizer = lambd * cvx.sum(cvx.multiply((1 - np.eye(n)), cvx.abs(K))) # Multiply arguments element-wise\n",
    "# regularizer = lambd * (cvx.norm(K,1) - cvx.trace(cvx.abs(K))) # Problem does not follow DCP rules.\n",
    "# print(regularizer)\n",
    "\n",
    "# log-likelihood \n",
    "log_likelihood = -cvx.log_det(K) + cvx.trace(sigmaHat * K)\n",
    "\n",
    "# Create the constraints. K is PSD and invertible, so K > 0 is always true.\n",
    "constraints = []\n",
    "\n",
    "# Form objective.\n",
    "objective = cvx.Minimize(log_likelihood + regularizer)\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cvx.Problem(objective, constraints)\n",
    "\n",
    "# assign values to lambda and compute the problem\n",
    "for i in range(TRIALS):\n",
    "    lambd.value = lambda_vals[i]\n",
    "    prob.solve()\n",
    "    optimal_objective_vals.append(prob.value)\n",
    "    log_likelihood_vals.append(log_likelihood.value)\n",
    "    number_non_zeros_list.append(np.sum(K.value > 1e-06))\n",
    "\n",
    "# print(\"status:\", prob.status)\n",
    "# print(\"optimal value p:\", prob.value)\n",
    "# print(\"optimal var K:\", K.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the curve.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[10,6])\n",
    "\n",
    "plt.plot(lambda_vals, optimal_objective_vals, label='optimal objective value')\n",
    "plt.plot(lambda_vals, log_likelihood_vals, label='log-likelihood ')\n",
    "plt.plot(lambda_vals, number_non_zeros_list, label='number of non-zeros ')\n",
    "\n",
    "plt.xlabel('$\\lambda$', fontsize=16)\n",
    "plt.ylabel('value', fontsize=16)\n",
    "# plt.xscale('log')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"optimal objective value, log-likelihood, and number of non-zeros as a function of $\\lambda$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\bullet$ Total Variation Denoising.** In this question we investigate the problem of signal denoising.\n",
    "Consider a signal $x \\in \\mathbb{R}^n$ corrupted by noise. We measure the corrupted signal $x_{corr}$ and wish to\n",
    "recover a good estimate $\\hat{x}$ of the original signal. To do this we solve the total variation denoising\n",
    "problem\n",
    "\\begin{align*}\n",
    "\\rm minimize~& \\lVert \\hat{x}-x_{corr} \\rVert_2^2 + \\lambda \\lVert D\\hat{x}\\rVert_1\n",
    "\\end{align*}\n",
    "where $D$ is the discrete derivative operator. Using the data supplied in [asgn5q3.pkl](https://wattlecourses.anu.edu.au/pluginfile.php/1761006/mod_resource/content/4/asgn5data.zip) write a cvx\n",
    "program to solve the above optimization problem. You can load and plot the corrupted signal\n",
    "using the following code:\n",
    "\n",
    ">\\# Python     \n",
    "import cvxpy as cvx     \n",
    "import numpy as np     \n",
    "import pickle     \n",
    "import matplotlib.pyplot as plt     \n",
    "x_corr = pickle.load(open('asgn5q3.pkl', 'rb'))     \n",
    "n = len(x_corr)     \n",
    "plt.plot(x_corr, linewidth=2)     \n",
    ">plt.show()     \n",
    "\n",
    "You should experiment with your code to find a \"good\" value for $\\lambda$. Include your source code and\n",
    "a plot of the recovered signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\cvxpy\\problems\\problem.py:614: RuntimeWarning: overflow encountered in long_scalars\n",
      "  if self.max_big_small_squared < big*small**2:\n",
      "c:\\python35\\lib\\site-packages\\cvxpy\\problems\\problem.py:615: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.max_big_small_squared = big*small**2\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "(1051) Out of space.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4f173dda7f21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mlambd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#     prob.solve(solver=cvx.SCS)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0moptimal_objective_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mloss_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\cvxpy\\problems\\problem.py\u001b[0m in \u001b[0;36msolve\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\cvxpy\\problems\\problem.py\u001b[0m in \u001b[0;36m_solve\u001b[1;34m(self, solver, ignore_dcp, warm_start, verbose, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverse_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solving_chain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         solution = self._solving_chain.solve_via_data(self, data, warm_start, verbose,\n\u001b[1;32m--> 361\u001b[1;33m                                                       kwargs)\n\u001b[0m\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solving_chain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py\u001b[0m in \u001b[0;36msolve_via_data\u001b[1;34m(self, problem, data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m    232\u001b[0m         return self.solver.solve_via_data(data, warm_start, verbose,\n\u001b[1;32m--> 233\u001b[1;33m                                           solver_opts, problem._solver_cache)\n\u001b[0m",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\cvxpy\\reductions\\solvers\\conic_solvers\\mosek_conif.py\u001b[0m in \u001b[0;36msolve_via_data\u001b[1;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputclist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputobjsense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmosek\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjsense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\mosek\\__init__.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6917\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6918\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getlasterror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6919\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6920\u001b[0m       \u001b[0m_trmcode_return_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6921\u001b[0m       \u001b[0m_trmcode_return_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrescode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_trmcode_return_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: (1051) Out of space."
     ]
    }
   ],
   "source": [
    "# Python\n",
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "from scipy.sparse import diags\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problem data and parameters.\n",
    "x_corr = pickle.load(open('asgn5q3.pkl', 'rb'))\n",
    "n = len(x_corr) # 300x1\n",
    "x_Hat = cvx.Variable((n,1))\n",
    "lambd = cvx.Parameter(nonneg=True)\n",
    "TRIALS = 50\n",
    "lambda_vals = np.logspace(-4, 1, TRIALS)\n",
    "\n",
    "D = diags([-1,1], [0,1], shape=(n-1,n)).toarray() # bidiagonal matrix\n",
    "# print(D)\n",
    "optimal_objective_vals = []\n",
    "loss_vals =[]\n",
    "regularizer_vals = []\n",
    " \n",
    "# loss = cvx.pnorm(x_Hat - x_corr, p=2)**2\n",
    "loss = cvx.norm(x_Hat - x_corr, 2)**2\n",
    "# regularizer = lambd * cvx.pnorm(cvx.matmul(D,x_Hat), 1)\n",
    "regularizer = cvx.norm(cvx.matmul(D,x_Hat), 1)\n",
    "# regularizer = lambd * cvx.norm(D*x_Hat, 1)\n",
    "\n",
    "# Create the constraints: There are no specified constraints for this problem.\n",
    "constraints = []\n",
    "\n",
    "# Form objective.\n",
    "objective = cvx.Minimize(loss + lambd * regularizer)\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cvx.Problem(objective, constraints)\n",
    "\n",
    "# assign values to lambda and compute the problem\n",
    "for i in range(TRIALS):\n",
    "    lambd.value = lambda_vals[i]\n",
    "    prob.solve(solver=cvx.SCS)\n",
    "#     prob.solve()\n",
    "    optimal_objective_vals.append(prob.value)\n",
    "    loss_vals.append(loss.value)\n",
    "    regularizer_vals.append(regularizer.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the curve.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot x_corr\n",
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.plot(x_corr, linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "# Plot x_Hat and x_corr\n",
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.plot(x_corr, linewidth=2, label='$x_{corr}$')\n",
    "plt.plot(x_Hat.value, linewidth=2, label='$\\hat{x}$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot trade-off curve\n",
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.plot(regularizer_vals, loss_vals, label='Optimal trade-off curve')\n",
    "\n",
    "plt.xlabel('$||\\hat{x}-x_{corr}||_2\")$', fontsize=16)\n",
    "plt.ylabel('$|| D\\hat{x}||_1$', fontsize=16)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Optimal trade-off curve between $||D\\hat{x}||_1$ and $||\\hat{x}-x_{corr}||$\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref for code\n",
    "@article{cvxpy_rewriting,\n",
    "  author  = {Akshay Agrawal, Robin Verschueren, Steven Diamond and Stephen Boyd},\n",
    "  title   = {A Rewriting System for Convex Optimization Problems},\n",
    "  journal = {Journal of Control and Decision},\n",
    "  year    = {2018},\n",
    "  volume  = {5},\n",
    "  number  = {1},\n",
    "  pages   = {42--60},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
